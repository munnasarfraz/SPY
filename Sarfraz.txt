h1. Data Transfer Automation Script Documentation

!https://d1.awsstatic.com/logos/aws-logo-lockups/poweredbyaws/PB_AWS_logo_RGB_stacked_REV_SQ.91cd4af40773cbfbd15577a3c2b8a346fe3e8fa2.png!

*Last updated:* {date}

h2. Overview

This Python-based automation script enables secure data transfer between EC2 instances using S3 as intermediate storage. It is built for large-scale, cross-environment transfers with support for service stopping/starting, splitting large archives, and multi-threaded S3 operations.

*Use Cases:*
- Data migration between AWS environments (e.g., DEV → UAT → PROD)
- Backups before critical updates
- Automated recovery replication
- CI/CD related environment cloning

*Benefits:*
- Reduces manual effort and errors
- Scales to large file sizes with split/merge logic
- Works via AWS SSM (no SSH key needed)
- Integrated error handling and logs

h2. Prerequisites

* AWS CLI installed and configured (with required profiles)
* IAM roles for EC2 with:
  * ssm:SendCommand
  * s3:PutObject, s3:GetObject
  * ec2:DescribeInstances
* `pigz`, `tar`, `split`, `cat` available on EC2s
* {Mermaid plugin installed in Confluence (if you want diagram rendering)}
* Lambda/Script execution permissions (if automated)
* CloudWatch logging enabled (recommended)

h2. Manual Setup Steps (If Applicable)

* Placeholder for setup guide: IAM roles, bucket creation, permissions.
* Create/verify S3 bucket `{your-bucket}` with lifecycle policy.
* Place script in automation environment (Lambda, local machine, EC2).
* Test `aws ssm send-command` manually from CLI to verify role access.

h2. Architecture Diagram

{mermaid}
graph TD
    A[Source Instance] -->|1. Stop Services| B[Compress Data]
    B --> C{Size > 5GB?}
    C -->|Yes| D[Split Files]
    C -->|No| E[Single Archive]
    D --> F[Upload Parts to S3]
    E --> F
    F --> G[Destination Instance]
    G -->|2. Download| H{Multiple Parts?}
    H -->|Yes| I[Combine Files]
    H -->|No| J[Extract Directly]
    I --> J
    J --> K[Start Services]
{mermaid}

h2. Technical Specifications

|| Category || Details ||
| Language | Python 3.9+ |
| AWS Services | EC2, S3, Systems Manager (SSM) |
| Compression | pigz (parallel gzip) |
| Libraries | boto3, concurrent.futures, logging, os |
| Threads | Configurable |

h2. Configuration Constants

{code:language=python}
SPLIT_ON = 5 * 1024 * 1024 * 1024  # 5GB
MNT_PATH = '/mnt'
SSM_TIMEOUT = 1800
SSM_INTERVAL = 5
{code}

h3. Input JSON Template

{code:language=json}
{
  "dry_run": false,
  "s3Bucket": "your-transfer-bucket",
  "services": [
    {
      "enabled": true,
      "name": "Service Name",
      "source": {
        "environment": "aws-profile",
        "instanceId": "i-1234567890",
        "path": "/source/path"
      },
      "destinations": [
        {
          "environment": "aws-profile",
          "instanceId": "i-0987654321",
          "path": "/destination/path"
        }
      ]
    }
  ]
}
{code}

h2. Workflow Details

h3. Source

1. Stop services (via SSM)
2. Compress data: `tar + pigz -p 4`
3. Split if >5GB: `split -b 5G`

h3. S3 Upload

{code}
aws s3 cp file s3://bucket/path/ --sse AES256
{code}

h3. Destination

1. Download using ThreadPoolExecutor
2. Combine parts (if needed): `cat part* > file.tar.gz`
3. Extract: `tar -xzf file.tar.gz`
4. Start services

h2. Error Handling

|| Error Type || Detection || Action ||
| Disk Space | `df -k` check | Abort with cleanup tip |
| SSM Fail | Status check | Retry 3x |
| S3 Error | CLI return code | Log & skip |
| Service Stop Fail | Process check | Warn and continue |

h2. Performance Tuning

{code:language=python}
SPLIT_SIZE = 1 * 1024 * 1024 * 1024
SSM_TIMEOUT = 3600
{code}

h2. Sample Report Output

{code:language=json}
{
  "status": "done",
  "results": [
    {
      "name": "Routing Engine",
      "status": "Success",
      "metrics": {
        "transfer_size_gb": 12.7,
        "compression_ratio": 3.2,
        "total_time_sec": 428
      },
      "details": {
        "source": "i-0591295a1783a221a",
        "destinations": [
          {
            "instance": "i-057d31b7dfd13887d",
            "throughput_mbps": 112.4
          }
        ]
      }
    }
  ]
}
{code}

h2. Troubleshooting

|| Symptom || Diagnosis || Solution ||
| SSM Timeout | CloudWatch | Increase timeout |
| Partial Transfer | S3 logs | Use transfer acceleration |
| Permission Errors | IAM Simulator | Add required permissions |

h2. Appendix

h3. A. Start/Stop Service Scripts

{code:bash}
# Start
/opt/engine/scripts/start-engine.sh

# Stop
/opt/engine/scripts/stop-engine.sh
{code}

h3. B. Python Requirements

{code}
boto3==1.26.0
concurrent-log-handler==0.9.20
{code}

h3. C. CloudWatch Monitoring Query

{code:language=sql}
STATS avg(duration) by bin(1h)
FILTER @message LIKE /Processing/
{code}

---

h3. Security Note

* Rotate IAM credentials regularly
* Review S3 policies post-deployment

h3. Maintenance

* Review this document quarterly
* Update script for AWS API/version changes







def verify_sizes(size_dict):
    logger.info("\n=== SIZE VERIFICATION ===")
    
    # Get all values from the dictionary
    sizes = list(size_dict.values())
    
    # Check if all values are equal (with tolerance for floating point differences)
    all_match = all(abs(sizes[0] - size) / sizes[0] < 0.01 for size in sizes)  # 1% tolerance
    
    # Log each item
    for instance, size in size_dict.items():
        logger.info(f"{instance}: {size} bytes")
    
    # Log comparison results
    if all_match:
        logger.info("\nALL INSTANCES HAVE MATCHING SIZES (within 1% tolerance)")
    else:
        logger.info("\nWARNING: SIZE MISMATCHES DETECTED")
        # Show differences
        base_size = sizes[0]
        for instance, size in size_dict.items():
            diff = size - base_size
            pct_diff = (diff / base_size) * 100
            logger.info(f"{instance} differs by {abs(diff)} bytes ({abs(pct_diff):.2f}%) from {base_size}")
    
    logger.info("=== END VERIFICATION ===\n")

# Example usage:
size_dict = {
    'source_xyz': 200,
    'destination_abc': 200,
    'destination_xyz': 201
}

verify_sizes(size_dict)


Here’s a compact paragraph version of icons for logs:

Use ✅ for success, ❌ for error or failure, ⚠️ for warnings, ℹ️ for informational messages, 🔄 for process started, ⏳ for in progress, 🟢 for completed, 🛑 for stopped, ⏭️ for skipped, and 🔁 for retrying. For deployment and builds, use 🚀 for deployment, 🛠️ for build started, 🧱✅ for build success, 🧱❌ for build failure, 🔙 for rollback, 🧪 for tests running, 🧪✅ for test passed, and 🧪❌ for test failed. For security and access logs, use 🔐 for authentication, 🚫 for unauthorized access, and 🔒 for permission denied. Infrastructure icons include 🖥️🟢 for server up, 🖥️🔴 for server down, 🗄️✅ for database connected, 🗄️❌ for database error, 📤 for file uploaded, 📥 for file downloaded, 💾 for backup, and ♻️ for restore. In AI or automation contexts, 🤖 denotes AI decision-making and 📜 represents scripts running. For monitoring, use 📈 for tracking metrics, 🚨 for alerts, and 📝 for log entries.

Let me know if you want this in a copy-paste-ready format like Python or JSON.


def download_and_extract_files(ssm_client, instance_id, s3_bucket, s3_prefix, zip_file_base, dest_path):
    try:
        # Create local destination path if not exists
        mkdir_cmd = f"mkdir -p {dest_path}"
        run_ssm_command(ssm_client, instance_id, mkdir_cmd)

        # Download files from S3
        download_cmd = f'aws s3 cp "s3://{s3_bucket}/{s3_prefix}/" "{MNT_PATH}/" --recursive --only-show-errors'
        run_ssm_command(ssm_client, instance_id, download_cmd)

        # Check if download was successful by verifying files exist
        list_files_cmd = f'ls "{MNT_PATH}/{zip_file_base}"* 2>/dev/null || echo "NO_FILES"'
        files_output = run_ssm_command_and_get_output(ssm_client, instance_id, list_files_cmd)
        
        if "NO_FILES" in files_output:
            error_msg = f"No files found for {zip_file_base} in S3 location s3://{s3_bucket}/{s3_prefix}/"
            logger.error(error_msg)
            return [False, error_msg]

        if IS_SPLIT:
            # Combine and extract split files
            split_prefix = f"{MNT_PATH}/{zip_file_base}.part."
            combined_file = f"{MNT_PATH}/{zip_file_base}.tar.gz"
            
            # Verify split parts exist
            list_parts_cmd = f"ls {split_prefix}* 2>/dev/null || echo 'NO_SPLIT_PARTS'"
            parts_output = run_ssm_command_and_get_output(ssm_client, instance_id, list_parts_cmd)
            
            if "NO_SPLIT_PARTS" in parts_output:
                error_msg = f"No split parts found for {zip_file_base} in {MNT_PATH}"
                logger.error(error_msg)
                return [False, error_msg]

            # Join split parts
            join_cmd = f"ls {split_prefix}* | sort -V | xargs cat > {combined_file}"
            run_ssm_command(ssm_client, instance_id, join_cmd)
            
            # Verify combined file was created
            verify_combined_cmd = f'[ -f "{combined_file}" ] && echo "EXISTS" || echo "MISSING"'
            verify_output = run_ssm_command_and_get_output(ssm_client, instance_id, verify_combined_cmd)
            
            if "MISSING" in verify_output:
                error_msg = f"Failed to combine split parts for {zip_file_base}"
                logger.error(error_msg)
                return [False, error_msg]

            # Remove split files
            cleanup_cmd = f'rm -f "{split_prefix}*"'
            run_ssm_command(ssm_client, instance_id, cleanup_cmd)
        else:
            combined_file = f"{MNT_PATH}/{zip_file_base}.tar.gz"
            
            # Verify single archive exists
            verify_archive_cmd = f'[ -f "{combined_file}" ] && echo "EXISTS" || echo "MISSING"'
            verify_output = run_ssm_command_and_get_output(ssm_client, instance_id, verify_archive_cmd)
            
            if "MISSING" in verify_output:
                error_msg = f"Archive file {combined_file} not found"
                logger.error(error_msg)
                return [False, error_msg]

        # Extract the tar.gz
        extract_cmd = f'cd {dest_path} && tar -xzf "{combined_file}"'
        run_ssm_command(ssm_client, instance_id, extract_cmd)
        
        # Verify extraction was successful
        verify_extract_cmd = f'[ "$(ls -A {dest_path})" ] && echo "NOT_EMPTY" || echo "EMPTY"'
        verify_output = run_ssm_command_and_get_output(ssm_client, instance_id, verify_extract_cmd)
        
        if "EMPTY" in verify_output:
            error_msg = f"Extraction failed - destination directory {dest_path} is empty"
            logger.error(error_msg)
            return [False, error_msg]

        # Remove archive after extraction
        rm_cmd = f'rm -f "{combined_file}"'
        run_ssm_command(ssm_client, instance_id, rm_cmd)

        logger.info(f"Successfully downloaded and extracted {zip_file_base} to {dest_path}")
        return [True, "Download and extraction completed successfully"]

    except Exception as e:
        error_msg = f"Failed to download/extract for {zip_file_base}: {str(e)}"
        logger.error(error_msg)
        return [False, error_msg]

def process_destination_instance(ssm_client, s3_client, instance_id, s3_bucket, service_folder, timestamp, dest_path, dry_run):
    try:
        bucket_size = 0
        start_time = time.time()
        if dry_run:
            logger.info(f"[{service_folder}] Dry-run enabled. Skipping download.")
            return {
                'status': 'Success', 
                'service': service_folder, 
                'message': 'Dry-run: Download skipped',
                'details': []
            }

        stop_status = service_via_ssm(ssm_client, instance_id, service_folder, 'stop')
        if not stop_status:
            return {
                'status': 'Error', 
                'service': service_folder, 
                'message': f'Failed to stop: {service_folder} services',
                'details': ['Service stop failed']
            }

        s3_prefix = f"temp/{timestamp}/{service_folder}"
        bucket_size = get_s3_bucket_size(s3_client, s3_bucket, s3_prefix)
        cleanup_directory(ssm_client, instance_id, dest_path)
        available_size_in_bytes = get_available_size_via_ssm(ssm_client, instance_id, dest_path)

        if available_size_in_bytes > bucket_size:
            download_success, download_message = download_and_extract_files(
                ssm_client, instance_id, s3_bucket, s3_prefix, 
                service_folder, dest_path
            )
        else:          
            difference_in_mb = (bucket_size - available_size_in_bytes) / (1024 * 1024)
            error_msg = f"Insufficient disk space: {difference_in_mb:.2f} MB needed"
            logger.error(f"Cleanup the instance [{instance_id}] at least [{difference_in_mb:.2f}] MB")
            return {
                'status': 'Error', 
                'service': service_folder, 
                'message': error_msg,
                'details': [error_msg]
            }

        start_status = service_via_ssm(ssm_client, instance_id, service_folder, 'start')
        if not start_status:
            return {
                'status': 'Error', 
                'service': service_folder, 
                'message': f'Failed to start: {service_folder} services',
                'details': ['Service start failed']
            }

        duration = time.time() - start_time
        if download_success:
            return {
                'status': 'Success',
                'service': service_folder,
                'message': f"Download and extraction completed for {service_folder}",
                'details': [download_message],
                'duration_seconds': duration
            }
        else:
            return {
                'status': 'Error',
                'service': service_folder,
                'message': f"Download failed for {service_folder}",
                'details': [download_message],
                'duration_seconds': duration
            }

    except Exception as e:
        logger.error(f"Error processing destination for service {service_folder}: {e}")
        return {
            'status': 'Error', 
            'service': service_folder, 
            'message': f"Error: {str(e)}",
            'details': [f"Unexpected error: {str(e)}"]
        }

def lambda_handler(event, context):
    # Add initial log with full event details
    logger.info(f"Script started with event: {json.dumps(event, indent=2)}")
    logger.info(f"Context: {context}")
    logger.info(f"Dry Run: {dry_run} | Multi-threaded: {multi_threaded}")
    
    # ... rest of your code ...
    
    # Add summary log at the end
    logger.info(f"Execution completed. Results summary: {json.dumps(results, indent=2)}")
    return {
        "status": "done",
        "results": results
    }

def process_source_instance(ssm_client, s3_client, instance_id, source_path, s3_bucket, service_folder, timestamp, dry_run):
    try:
        logger.info(f"[{service_folder}] Starting source processing for instance {instance_id}")
        logger.debug(f"Source path: {source_path}, S3 bucket: {s3_bucket}, Timestamp: {timestamp}")
        
        # ... existing code ...
        
        logger.info(f"[{service_folder}] Directory size: {dir_size} bytes ({dir_size/1024/1024:.2f} MB)")
        
        if dry_run:
            logger.warning(f"[{service_folder}] DRY RUN - Skipping actual upload")
            return {'status': 'Success', 'service': service_folder, 'message': 'Dry-run: Upload skipped'}

        # Add detailed logging for service stop/start
        logger.info(f"[{service_folder}] Stopping service...")
        stop_status = service_via_ssm(ssm_client, instance_id, service_folder, 'stop')
        if not stop_status:
            logger.error(f"[{service_folder}] Failed to stop service")
            return {'status': 'Error', 'service': service_folder, 'message': f'Failed to stop: {service_folder} services'}
        logger.info(f"[{service_folder}] Service stopped successfully")

        # ... rest of your code ...

    except Exception as e:
        logger.exception(f"[{service_folder}] Critical error in process_source_instance")  # This logs the full traceback
        return {'status': 'Error', 'service': service_folder, 'message': f"Error: {str(e)}"}

def process_destination_instance(ssm_client, s3_client, instance_id, s3_bucket, service_folder, timestamp, dest_path, dry_run):
    try:
        logger.info(f"[{service_folder}] Starting destination processing for instance {instance_id}")
        logger.debug(f"Destination path: {dest_path}, S3 bucket: {s3_bucket}, Timestamp: {timestamp}")
        
        # ... existing code ...
        
        logger.info(f"[{service_folder}] S3 bucket size: {bucket_size} bytes ({bucket_size/1024/1024:.2f} MB)")
        
        # Add detailed logging for cleanup
        logger.info(f"[{service_folder}] Cleaning up destination directory {dest_path}")
        cleanup_directory(ssm_client, instance_id, dest_path)
        
        # ... rest of your code ...

    except Exception as e:
        logger.exception(f"[{service_folder}] Critical error in process_destination_instance")
        return {'status': 'Error', 'service': service_folder, 'message': f"Error: {str(e)}"}

def service_via_ssm(ssm_client, instance_id, service, action):
    try:
        logger.info(f"Executing {action} for {service} on instance {instance_id}")
        start_time = time.time()
        
        # ... existing code ...
        
        logger.info(f"Service {service} {action} completed in {duration:.2f} seconds")
        logger.debug(f"Command output: {output}")
        
        return True
    except Exception as e:
        logger.error(f"Failed to {action} service {service} on instance {instance_id}: {str(e)}")
        return False

def run_ssm_command_and_get_output(ssm_client, instance_id, command):
    try:
        logger.debug(f"Executing SSM command on {instance_id}: {command}")
        response = ssm_client.send_command(
            InstanceIds=[instance_id],
            DocumentName="AWS-RunShellScript",
            Parameters={'commands': [command]},
            TimeoutSeconds=SSM_TIMEOUT,
        )
        command_id = response['Command']['CommandId']
        
        for _ in range(int(SSM_TIMEOUT / SSM_INTERVAL)):
            time.sleep(SSM_INTERVAL)
            output = ssm_client.get_command_invocation(CommandId=command_id, InstanceId=instance_id)
            if output['Status'] in ['Success', 'Failed', 'TimedOut', 'Cancelled']:
                if output['Status'] != 'Success':
                    logger.error(f"SSM command failed on {instance_id}: {output['StandardErrorContent']}")
                    raise Exception(output['StandardErrorContent'])
                logger.debug(f"SSM command succeeded on {instance_id}")
                return output['StandardOutputContent']
    except Exception as e:
        logger.error(f"SSM command execution failed on {instance_id}: {str(e)}")
        raise




Exception ignored in: <function _DeleteDummyThreadOnDel.__del__ at 0x0000025D62287E20>
Traceback (most recent call last):
  File "c:\Users\atp1msa\AppData\Local\Programs\Python\Python313\Lib\threading.py", line 1383, in __del__
TypeError: 'NoneType' object does not support the context manager protocol
Exception ignored in: <function _DeleteDummyThreadOnDel.__del__ at 0x0000025D62287E20>
Traceback (most recent call last):
  File "c:\Users\atp1msa\AppData\Local\Programs\Python\Python313\Lib\threading.py", line 1383, in __del__
TypeError: 'NoneType' object does not support the context manager protocol


import boto3
import time
import logging
import os
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor

'''Constants----------------------------------------------------------------------------'''
SPLIT_ON = 5 * 1024 * 1024 * 1024  # 2 GB
SPLIT_SIZE = 5 * 1024 * 1024 * 1024  # 1 GB
IS_SPLIT = False
MNT_PATH = '/mnt'
SSM_TIMEOUT = 1800
SSM_INTERVAL = 5
'''-------------------------------------------------------------------------------------'''

'''log setup----------------------------------------------------------------------------'''
logging.basicConfig(level=logging.INFO) # Setup logging
logger = logging.getLogger(__name__)
'''-------------------------------------------------------------------------------------'''

'''get_session------------------------------------------------------------------------------
Retrieve or create a boto3 session for the given profile name.
Args:    profile_name (str): The name of the AWS profile to use.
Returns: boto3.Session: A boto3 session associated with the specified profile.
------------------------------------------------------------------------------------------'''
session_cache = {}
def get_session(profile_name):
    if profile_name not in session_cache:
        session_cache[profile_name] = boto3.Session(profile_name=profile_name)
    return session_cache[profile_name]

'''lambda_handler -> normal python function--------------------------------------------------
AWS Lambda function to process services listed in the event parameter.
Parameters: event (dict): Contains input data for the json:  context (object): Not used  
Returns:    dict: Contains the overall status and results of the execution.
------------------------------------------------------------------------------------------'''
def lambda_handler(event, context):
    dry_run = event.get('dry_run', False)
    multi_threaded = True
    s3_bucket = event['s3Bucket']
    services = event.get("services", [])
    timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
    logger.info(f"Script started | Dry Run: {dry_run} | Multi-threaded: {multi_threaded}")
    results = []
    for service in services:
        if not service.get('enabled', False):
            logger.info(f"Skipping disabled service: {service['name']}")
            continue
        service_name = service['name']
        start_time = time.time()
        source_env = service['source']['environment']

        logger.info(f"Processing service: {service['name']}")
        service_result = {
            "name": service['name'],
            "status": "Pending",
            "upload_result": None,
            "download_results": []
        }
        source_session = get_session(source_env)
        source_ssm_client = source_session.client('ssm')
        source_s3_client = source_session.client('s3')
        source_instance = service['source']['instanceId']
        source_path = service['source']['path']

        upload_result = process_source_instance(
            source_ssm_client, source_s3_client,
            source_instance, source_path,
            s3_bucket, service_name, timestamp, dry_run
        )
        service_result["upload_result"] = upload_result

        if upload_result['status'] != 'Success':
            logger.error(f"Upload failed for service {service['name']}")
            service_result['status'] = 'Error'
            service_result['message'] = 'Upload failed; skipping download.'
            results.append(service_result)
            continue

        all_download_success = True

        def thread_wrapper(dest_ssm_client, dest_s3_client, dest_instance, s3_bucket, service_name, timestamp, dest_path, dry_run):
            logger.info(f"[{dest_instance}] Starting download and extract")
            result = process_destination_instance(
                dest_ssm_client, dest_s3_client,
                dest_instance, s3_bucket,
                service_name, timestamp, dest_path, dry_run
            )
            logger.info(f"[{dest_instance}] Finished with status: {result.get('status')}")
            return result

        if multi_threaded:
            logger.info(f"Using MULTI-THREADED mode for service: {service['name']}")
            with ThreadPoolExecutor(max_workers=len(service['destinations'])) as executor:
                futures = []
                for destination in service['destinations']:
                    dest_env = destination['environment']
                    dest_session = get_session(dest_env)
                    dest_ssm_client = dest_session.client('ssm')
                    dest_s3_client = dest_session.client('s3')
                    dest_instance = destination['instanceId']
                    dest_path = destination['path']

                    logger.info(f"Submitting thread for {dest_instance}")
                    future = executor.submit(
                        thread_wrapper,
                        dest_ssm_client, dest_s3_client,
                        dest_instance, s3_bucket,
                        service['name'], timestamp,
                        dest_path, dry_run
                    )
                    futures.append((future, destination))

                for future, destination in futures:
                    dest_instance = destination['instanceId']
                    try:
                        download_result = future.result()
                        logger.info(f"Completed thread for {dest_instance} with status: {download_result['status']}")
                    except Exception as e:
                        logger.error(f"Thread failed for {dest_instance}: {e}")
                        download_result = {'status': 'Error', 'message': f'Thread exception: {e}'}

                    service_result['download_results'].append({
                        **destination,
                        **download_result
                    })
                    if download_result['status'] != 'Success':
                        all_download_success = False
        else:
            logger.info(f"Using SEQUENTIAL mode for service: {service['name']}")
            for destination in service['destinations']:
                dest_env = destination['environment']
                dest_session = get_session(dest_env)
                dest_ssm_client = dest_session.client('ssm')
                dest_s3_client = dest_session.client('s3')

                dest_instance = destination['instanceId']
                dest_path = destination['path']

                logger.info(f"Starting sequential download for {dest_instance}")
                try:
                    download_result = process_destination_instance(
                        dest_ssm_client, dest_s3_client,
                        dest_instance, s3_bucket,
                        service_name, timestamp, dest_path, dry_run
                    )
                except Exception as e:
                    logger.error(f"Sequential error for {dest_instance}: {e}")
                    download_result = {'status': 'Error', 'message': f'Sequential exception: {e}'}

                logger.info(f"Completed sequential download for {dest_instance} with status: {download_result['status']}")
                service_result['download_results'].append({
                    **destination,
                    **download_result
                })
                if download_result['status'] != 'Success':
                    all_download_success = False

        service_result['status'] = 'Success' if all_download_success else 'Error'
        service_result['message'] = 'All destinations completed' if all_download_success else 'Some downloads failed'
        results.append(service_result)

    logger.info("Execution completed")
    return {
        "status": "done",
        "results": results
    }

'''service_via_ssm---------------------------------------------------------------------------
Executes specified action (start/stop) on a given service using AWS Systems Manager (SSM).
Parameters: ssm_client (boto3.client): The SSM client to execute commands.  instance_id (str): The ID of the instance where the service is running.
            service (str): The name of the service to be managed.   action (str): The action to perform on the service ('start' or 'stop').
Returns:    bool: True if the command executed successfully, False otherwise.
------------------------------------------------------------------------------------------'''
def service_via_ssm(ssm_client, instance_id, service, action):
    # Define the commands for stopping and starting services
    commands = {
        'stop': {
            'Journey Engine': 'sudo -u engineadmin /opt/atpco/engine/scripts/maint_nl/eng-stop-journeyengine.sh',
            'Routings Engine': 'sudo -u engineadmin /opt/atpco/engine/scripts/maint_nl/eng-stop-routings.sh'
        },
        'start': {
            'Journey Engine1': 'sudo -u engineadmin /opt/atpco/engine/scripts/maint_nl/eng-start-journeyengine.sh',
            'Routings Engine': 'sudo -u engineadmin /opt/atpco/engine/scripts/maint_nl/eng-start-routings.sh'
        }
    }
    command = commands[action].get(service) # Get the command for the specified service and action
    try:
        if command:
            logging.info(f"Executing {action} command for {service}: {command}")           
            start_time = time.time() # Capture the start time
            status, output = run_ssm_command_and_get_multiple_output(ssm_client, instance_id, command)
            end_time = time.time() # Capture the end time
            duration = end_time - start_time # Calculate the duration
        
            logging.info(f"Output for {service}: {output}")
            logging.info(f"Duration for {action} command for {service}: {duration:.2f} seconds")
            
            if status == 'Success': # Check if the command was executed successfully 
                logging.info(f"{action.capitalize()} command for {service} executed successfully.")
                return True
            else:
                logging.error(f"Failed to execute {action} command for {service}. Status: {output}")
                return False
        else:
            logging.error(f"No command found for {service} with action {action}.")
            return False
    except Exception as e:
        logging.error(f"Failed to execute {action} command for {service}. error: {e}")
        return False
    
'''get_available_size_via_ssm----------------------------------------------------------------
Retrieve the available size at the specified path on an instance using AWS Systems Manager (SSM).
Parameters:     ssm_client (boto3.client): The SSM client to execute commands.  instance_id (str): The ID of the instance where the directory is located.   source_path (str): The path to the directory on the instance.
Returns:int:    The available size in bytes at the specified path. Returns 0 if an error occurs.
------------------------------------------------------------------------------------------'''
def get_available_size_via_ssm(ssm_client, instance_id, source_path):
    mkdir_cmd = f"mkdir -p {source_path}"
    run_ssm_command(ssm_client, instance_id, mkdir_cmd)
    try:
        cmd = f"df -k {source_path} | tail -1 | awk '{{print $4}}'"
        output = run_ssm_command_and_get_output(ssm_client, instance_id, cmd)
        size_in_bytes = int(output.strip()) * 1024
        return size_in_bytes
    except:
        return 0

'''process_source_instance-------------------------------------------------------------------
Processes the source instance, checking available disk space, compressing or splitting files, 
uploading to S3, and restarting services using AWS Systems Manager (SSM).
Parameters:     ssm_client (boto3.client): The SSM client to execute commands.   s3_client (boto3.client): The S3 client to upload files.    instance_id (str): The ID of the source instance.   source_path (str): The path to the directory on the source instance.
                s3_bucket (str): The name of the S3 bucket for uploads. service_folder (str): The name of the service folder.   timestamp (str): The current timestamp for naming files.    dry_run (bool): Indicates if the function should run in dry-run mode.
Returns:        dict: Contains the status, service name, message, and duration of the upload process.
------------------------------------------------------------------------------------------'''
def process_source_instance(ssm_client, s3_client, instance_id, source_path, s3_bucket, service_folder, timestamp, dry_run):
    try:
        start_time = time.time()
        dir_size = get_directory_size_via_ssm(ssm_client, instance_id, source_path)
        logger.info(f"[{service_folder}] Directory size: {dir_size} bytes")

        if dry_run:
            logger.info(f"[{service_folder}] Dry-run enabled. Skipping upload.")
            return {'status': 'Success', 'service': service_folder, 'message': 'Dry-run: Upload skipped'}

        stop_status = service_via_ssm(ssm_client, instance_id, service_folder, 'stop') #stop services 
        if not stop_status:
            return {'status': 'Error', 'service': service_folder, 'message': f'failled to stop : {service_folder} services'}
 
        available_size_in_bytes = get_available_size_via_ssm(ssm_client, instance_id, source_path)
        if available_size_in_bytes > dir_size:
            if dir_size > SPLIT_ON:
                global IS_SPLIT
                IS_SPLIT = True
                split_files = split_directory_via_ssm(ssm_client, instance_id, source_path, service_folder)
            else:
                split_files = compress_directory_via_ssm(ssm_client, instance_id, source_path, service_folder)
        else:
            logger.warning(f"Disk space issue - available disk space [{available_size_in_bytes}] expected disk space: {dir_size} bytes")
            difference_in_bytes = dir_size - available_size_in_bytes
            difference_in_mb = difference_in_bytes / (1024 * 1024)
            logger.error(f"Cleanup the instance [{instance_id}] at least [{difference_in_mb:.2f}] MB")
            return {'status': 'Error', 'service': service_folder, 'message': f'Insufficient disk space: {difference_in_mb:.2f} MB needed'}

        start_status = service_via_ssm(ssm_client, instance_id, service_folder, 'start') #start services
        if not start_status:
            return {'status': 'Error', 'service': service_folder, 'message': f'failled to start : {service_folder} services'}
 
        if not split_files:
            return {'status': 'Error', 'service': service_folder, 'message': 'No archive created'}

        s3_prefix = f"temp/{timestamp}/{service_folder}"
        upload_results = [upload_file_to_s3_via_ssm(ssm_client, instance_id, f, s3_bucket, s3_prefix) for f in split_files]

        cleanup_temp_files(ssm_client, instance_id, split_files)    # Cleanup temp files

        if not all(upload_results):
            return {'status': 'Error', 'service': service_folder, 'message': 'Upload failed'}

        duration = time.time() - start_time
        return {'status': 'Success', 'service': service_folder, 'message': 'Upload completed', 'duration_seconds': duration}

    except Exception as e:
        if 'The SSO session associated with this profile has expired or is otherwise invalid. To refresh this SSO session run aws sso login with the corresponding profile.' in str(e):
            logger.error(f"To log in to AWS SSO using the command line, command: [aws sso login --profile env]")
        else:
            logger.error(f"Error processing source for service {service_folder}: {e}")
        return {'status': 'Error', 'service': service_folder, 'message': 'sso login failled' f"Error: {e}"}
    
'''process_destination_instance------------------------------------------------------------
Processes the destination instance by stopping services, checking available disk space, downloading and extracting files from S3, 
and restarting services using AWS Systems Manager (SSM).
Parameters:     ssm_client (boto3.client): The SSM client to execute commands.  s3_client (boto3.client): The S3 client to download files.
                instance_id (str): The ID of the destination instance.  s3_bucket (str): The name of the S3 bucket for downloads.   service_folder (str): The name of the service folder.
                timestamp (str): The current timestamp for naming files.    dest_path (str): The path to the directory on the destination instance. dry_run (bool): Indicates if the function should run in dry-run mode.
Returns:        dict: Contains the status, service name, message, and duration of the download process.
------------------------------------------------------------------------------------------'''
def process_destination_instance(ssm_client, s3_client, instance_id, s3_bucket, service_folder, timestamp, dest_path, dry_run):
    try:
        bucket_size = 0
        start_time = time.time()
        if dry_run:
            logger.info(f"[{service_folder}] Dry-run enabled. Skipping download.")
            return {'status': 'Success', 'service': service_folder, 'message': 'Dry-run: Download skipped'}

        stop_status = service_via_ssm(ssm_client, instance_id, service_folder, 'stop') #stop services 
        if not stop_status:
            return {'status': 'Error', 'service': service_folder, 'message': f'failled to stop : {service_folder} services'}
 
        s3_prefix = f"temp/{timestamp}/{service_folder}"
        bucket_size = get_s3_bucket_size(s3_client, s3_bucket, s3_prefix)
        #available_size_in_bytes = get_available_size_via_ssm(ssm_client, instance_id, dest_path)
        cleanup_directory(ssm_client, instance_id, dest_path)
        available_size_in_bytes = get_available_size_via_ssm(ssm_client, instance_id, dest_path)

        if available_size_in_bytes > bucket_size :
            download_results = download_and_extract_files(ssm_client, instance_id, s3_bucket, s3_prefix, service_folder, dest_path)
        else:          
            logger.warning(f"Disk space issue - available disk space [{available_size_in_bytes}] expected disk space: {bucket_size} bytes")
            difference_in_bytes = bucket_size - available_size_in_bytes
            difference_in_mb = difference_in_bytes / (1024 * 1024)
            logger.error(f"Cleanup the instance [{instance_id}] at least [{difference_in_mb:.2f}] MB")
            return {'status': 'Error', 'service': service_folder, 'message': f"Insufficient disk space: {difference_in_mb:.2f} MB needed"}
 
        start_status = service_via_ssm(ssm_client, instance_id, service_folder, 'start')    #start services
        if not start_status:
            return {'status': 'Error', 'service': service_folder, 'message': f'failled to start : {service_folder} services'}
         
        duration = time.time() - start_time
        return {
            'status': 'Success' if all(download_results) else 'Error',
            'service': service_folder,
            'message': f"Download and extraction completed for {service_folder}",
            'duration_seconds': duration
        }

    except Exception as e:
        logger.error(f"Error processing destination for service {service_folder}: {e}")
        return {'status': 'Error', 'service': service_folder, 'message': f"Error: {e}"}

'''run_ssm_command_and_get_output------------------------------------------------------------
Executes a shell command on an instance using AWS Systems Manager (SSM) and retrieves the output.
Parameters: ssm_client (boto3.client): The SSM client to execute commands. instance_id (str): The ID of the instance where the command is executed. command (str): The shell command to execute.
Returns:    str: The standard output content of the executed command.
Raises:     Exception: If the command execution fails or does not succeed.
------------------------------------------------------------------------------------------'''
def run_ssm_command_and_get_output(ssm_client, instance_id, command):
    response = ssm_client.send_command(
        InstanceIds=[instance_id],
        DocumentName="AWS-RunShellScript",
        Parameters={'commands': [command]},
        TimeoutSeconds=SSM_TIMEOUT,
    )
    command_id = response['Command']['CommandId']
    for _ in range(int(SSM_TIMEOUT / SSM_INTERVAL)):
        time.sleep(SSM_INTERVAL)
        output = ssm_client.get_command_invocation(CommandId=command_id, InstanceId=instance_id)
        if output['Status'] in ['Success', 'Failed', 'TimedOut', 'Cancelled']:
            if output['Status'] != 'Success':
                raise Exception(output['StandardErrorContent'])
            return output['StandardOutputContent']

'''run_ssm_command_and_get_multiple_output---------------------------------------------------
Executes a shell command on an instance using AWS Systems Manager (SSM) and retrieves the status and output.
Parameters: ssm_client (boto3.client): The SSM client to execute commands.  instance_id (str): The ID of the instance where the command is executed.    command (str): The shell command to execute.
Returns:    tuple: Contains the status of the command execution and the standard output content.
Raises:     Exception: If the command execution fails or does not succeed.
------------------------------------------------------------------------------------------'''
def run_ssm_command_and_get_multiple_output(ssm_client, instance_id, command):
    response = ssm_client.send_command(
        InstanceIds=[instance_id],
        DocumentName="AWS-RunShellScript",
        Parameters={'commands': [command]},
        TimeoutSeconds=SSM_TIMEOUT,
    )
    command_id = response['Command']['CommandId']
    for _ in range(int(SSM_TIMEOUT / SSM_INTERVAL)):
        time.sleep(SSM_INTERVAL)
        output = ssm_client.get_command_invocation(CommandId=command_id, InstanceId=instance_id)
        if output['Status'] in ['Success', 'Failed', 'TimedOut', 'Cancelled']:
            if output['Status'] != 'Success':
                raise Exception(output['StandardErrorContent'])
            return output['Status'], output['StandardOutputContent']
        
'''run_ssm_command---------------------------------------------------------------------------
Executes a shell command on an instance using AWS Systems Manager (SSM).
Parameters: ssm_client (boto3.client): The SSM client to execute commands.  instance_id (str): The ID of the instance where the command is executed.    command (str): The shell command to execute.
Returns:    None
Raises:     Exception: If the command execution fails or does not succeed.
------------------------------------------------------------------------------------------'''
def run_ssm_command(ssm_client, instance_id, command):
    response = ssm_client.send_command(
        InstanceIds=[instance_id],
        DocumentName="AWS-RunShellScript",
        Parameters={'commands': [command]},
        TimeoutSeconds=SSM_TIMEOUT,
    )
    command_id = response['Command']['CommandId']
    for _ in range(int(SSM_TIMEOUT / SSM_INTERVAL)):
        time.sleep(SSM_INTERVAL)
        output = ssm_client.get_command_invocation(CommandId=command_id, InstanceId=instance_id)
        if output['Status'] in ['Success', 'Failed', 'TimedOut', 'Cancelled']:
            if output['Status'] != 'Success':
                raise Exception(output['StandardErrorContent'])
            return

'''get_directory_size_via_ssm----------------------------------------------------------------
Retrieves the size of a directory on an instance using AWS Systems Manager (SSM).
Parameters: ssm_client (boto3.client): The SSM client to execute commands.  instance_id (str): The ID of the instance where the directory is located.   source_path (str): The path to the directory on the instance.
Returns:    int: The size of the directory in bytes.
------------------------------------------------------------------------------------------'''
def get_directory_size_via_ssm(ssm_client, instance_id, source_path):
    cmd = f"du -sb {source_path} | cut -f1"
    output = run_ssm_command_and_get_output(ssm_client, instance_id, cmd)
    return int(output.strip())

'''split_directory_via_ssm--------------------------------------------------------------------
Compresses and splits a directory into smaller parts on an instance using AWS Systems Manager (SSM).
Parameters: ssm_client (boto3.client): The SSM client to execute commands.instance_id (str): The ID of the instance where the directory is located.
            source_path (str): The path to the directory on the instance.   zip_file_base (str): The base name for the compressed and split files.
Returns:    list: A list of paths to the split parts of the compressed archive.
------------------------------------------------------------------------------------------'''
def split_directory_via_ssm(ssm_client, instance_id, source_path, zip_file_base):
    tar_path = f"{MNT_PATH}/{zip_file_base}.tar.gz"
    split_prefix = f"{MNT_PATH}/{zip_file_base}.part."
 
    cmd_compress = f"cd {source_path} && tar -cf - . | pigz -p 4 > {tar_path}"  # Step 1: Create compressed archive
    run_ssm_command(ssm_client, instance_id, cmd_compress)
 
    cmd_split = f"split -b {SPLIT_SIZE} -d -a 3 {tar_path} {split_prefix}"      # Step 2: Split it
    run_ssm_command(ssm_client, instance_id, cmd_split)
 
    list_cmd = f"ls {split_prefix}*"                                            # Step 3: List split parts
    output = run_ssm_command_and_get_output(ssm_client, instance_id, list_cmd)
    return output.strip().split('\n')
'''---------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------'''
def compress_directory_via_ssm(ssm_client, instance_id, source_path, zip_file_base):
    cmd = f'cd {source_path} && tar -cf - . | pigz -p 4 > "{MNT_PATH}/{zip_file_base}.tar.gz"'
    run_ssm_command(ssm_client, instance_id, cmd)
    return [f"{MNT_PATH}/{zip_file_base}.tar.gz"]

'''---------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------'''
def upload_file_to_s3_via_ssm(ssm_client, instance_id, file_path, s3_bucket, s3_prefix):
    filename = os.path.basename(file_path)
    s3_key = f"{s3_prefix}/{filename}"
    cmd = f'aws s3 cp "{file_path}" "s3://{s3_bucket}/{s3_key}" --only-show-errors'
    try:
        run_ssm_command(ssm_client, instance_id, cmd)
        logger.info(f"Uploaded {filename} to s3://{s3_bucket}/{s3_key}")
        return True
    except Exception as e:
        logger.error(f"Upload failed for {filename}: {e}")
        return False
'''---------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------'''
def cleanup_temp_files(ssm_client, instance_id, file_paths):
    for file_path in file_paths:
        cmd = f'rm -f "{file_path}"'
        try:
            run_ssm_command(ssm_client, instance_id, cmd)
            logger.info(f"Cleaned up {file_path}")
        except Exception as e:
            logger.warning(f"Failed to clean up {file_path}: {e}")
'''---------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------'''
def cleanup_directory(ssm_client, instance_id, dir_paths):
    cmd = f'rm -rf "{dir_paths}/*"'
    try:
        run_ssm_command(ssm_client, instance_id, cmd)
        logger.info(f"Successfully cleaned up {dir_paths}")
    except ssm_client.exceptions.CommandFailed as e:
        logger.error(f"SSM command failed for {dir_paths}: {e}")
    except Exception as e:
        logger.warning(f"Failed to clean up {dir_paths}: {e}")
'''---------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------'''
def get_s3_bucket_size(s3_client, bucket_name, prefix=''):
    paginator = s3_client.get_paginator('list_objects_v2')
    total_size = 0

    for page in paginator.paginate(Bucket=bucket_name, Prefix=prefix):
        for obj in page.get('Contents', []):
            total_size += obj['Size']
    return total_size

'''---------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------'''
def download_and_extract_files(ssm_client, instance_id, s3_bucket, s3_prefix, zip_file_base, dest_path):
    try:
        # Create local destination path if not exists
        mkdir_cmd = f"mkdir -p {dest_path}"
        run_ssm_command(ssm_client, instance_id, mkdir_cmd)

        # Download files from S3
        download_cmd = f'aws s3 cp "s3://{s3_bucket}/{s3_prefix}/" "{MNT_PATH}/" --recursive --only-show-errors'
        run_ssm_command(ssm_client, instance_id, download_cmd)

        if IS_SPLIT:
        # Combine and extract split files or extract single file
            split_prefix = f"{MNT_PATH}/{zip_file_base}.part."
            combined_file = f"{MNT_PATH}/{zip_file_base}.tar.gz"
            list_parts_cmd = f"ls {split_prefix}* 2>/dev/null"
            parts_output = run_ssm_command_and_get_output(ssm_client, instance_id, list_parts_cmd)

        #if parts_output.strip():
        if IS_SPLIT:
            # Join split parts
            join_cmd = f"ls {split_prefix}* | sort -V | xargs cat > {combined_file}"

            run_ssm_command(ssm_client, instance_id, join_cmd)
            cleanup_cmd = f'rm -f "{split_prefix}*"' # Remove split files
            run_ssm_command(ssm_client, instance_id, cleanup_cmd)
        else:
            combined_file = f"{MNT_PATH}/{zip_file_base}.tar.gz"

        # Extract the tar.gz
        extract_cmd = f'cd {dest_path} && tar -xzf "{combined_file}"'
        run_ssm_command(ssm_client, instance_id, extract_cmd)

        # Remove archive after extraction
        rm_cmd = f'rm -f "{combined_file}"'
        run_ssm_command(ssm_client, instance_id, rm_cmd)

        logger.info(f"Downloaded and extracted {zip_file_base} to {dest_path}")
        return [True]

    except Exception as e:
        logger.error(f"Failed to download/extract for {zip_file_base}: {e}")
        return [False]

'''---------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------'''
if __name__ == "__main__":
    test_event = {
        "services": [
            {
                "enabled": True,
                "name": "Routings Engine",
                "source": {
                    "environment": "ppj",
                    "instanceId": "i-0591295a1783a221a",
                    "path": "/opt/atpco/engine/db/neo4j/chgdetroutings/Routings/"
                },
                "destinations": [
                    {
                        "environment": "ppj",
                        "instanceId": "i-057d31b7dfd13887d",
                        "path": "/opt/atpco/engine/db/neo4j/chgdetroutings/Routings_1"
                    },
                    {
                        "environment": "ppj",
                        "instanceId": "i-057d31b7dfd13887d",
                        "path": "/opt/atpco/engine/db/neo4j/chgdetroutings/Routings_1"
                    }
                ]
            },
            {
                "enabled": False,
                "name": "xyz",
                "source": {
                    "environment": "ppj",
                    "instanceId": "i-057d31b7dfd13887d",
                    "path": "/opt/atpco/engine/db/neo4j/chgdet/Engine_tmp"
                },
                "destinations": [
                    {
                        "environment": "ppj",
                        "instanceId": "i-057d31b7dfd13887d",
                        "path": "/opt/atpco/engine/db/neo4j/chgdet/Engine_tmp_1"
                    },
                    {
                        "environment": "engu",
                        "instanceId": "i-0703c925af9018ad3",
                        "path": "/opt/atpco/engine/db/neo4j/chgdetroutings/Routings_1"
                    }
                ]
            },
            {
                "enabled": False,
                "name": "journey1",
                "source": {
                    "environment": "engu",
                    "instanceId": "i-0703c925af9018ad3",
                    "path": "/opt/atpco/engine/db/neo4j/chgdetroutings/Routings_1"
                },
                "destinations": [
                    {
                        "environment": "ppj",
                        "instanceId": "i-057d31b7dfd13887d",
                        "path": "/opt/atpco/engine/db/neo4j/chgdet/Engine_tmp_1"
                    }
                ]
            }

        ],
        "s3Bucket": "ppj-transfer-bucket",
        "dryRun": False 
    }

    ret_val = lambda_handler(test_event, None)
    print(ret_val)
