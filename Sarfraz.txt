def delete_s3_prefix_via_ssm(ssm_client, instance_id, bucket_name, prefix=None):
    try:
        logger.info(f"üóëÔ∏è Deleting all objects in s3://{bucket_name} via SSM on {instance_id}")
        cmd = f'aws s3 rm "s3://{bucket_name}/" --recursive --only-show-errors'
        run_ssm_command(ssm_client, instance_id, cmd)
        logger.info(f"‚úÖ Successfully deleted all objects in s3://{bucket_name}")
        return True
    except Exception as e:
        logger.error(f"‚ùå Failed to delete objects in s3://{bucket_name}: {str(e)}", exc_info=True)
        return False

{
    "Effect": "Allow",
    "Action": ["s3:DeleteObject", "s3:ListBucket"],
    "Resource": [
        "arn:aws:s3:::your-bucket-name",
        "arn:aws:s3:::your-bucket-name/*"
    ]
}


'''delete_s3_prefix_objects-----------------------------------------------------------------
Deletes all objects in the S3 bucket under the specified prefix.
Parameters: s3_client, bucket_name, prefix
Returns: bool: True if deletion succeeds, False otherwise.
------------------------------------------------------------------------------------------'''
def delete_s3_prefix_objects(s3_client, bucket_name, prefix):
    try:
        logger.info(f"üóëÔ∏è Deleting objects in s3://{bucket_name}/{prefix}")
        paginator = s3_client.get_paginator('list_objects_v2')
        pages = paginator.paginate(Bucket=bucket_name, Prefix=prefix)
        
        objects_to_delete = {'Objects': []}
        for page in pages:
            if 'Contents' in page:
                for obj in page['Contents']:
                    objects_to_delete['Objects'].append({'Key': obj['Key']})
        
        if objects_to_delete['Objects']:
            s3_client.delete_objects(Bucket=bucket_name, Delete=objects_to_delete)
            logger.info(f"‚úÖ Successfully deleted objects in s3://{bucket_name}/{prefix}")
        else:
            logger.info(f"‚ÑπÔ∏è No objects found in s3://{bucket_name}/{prefix}")
        return True
    except Exception as e:
        logger.error(f"‚ùå Failed to delete objects in s3://{bucket_name}/{prefix}: {str(e)}", exc_info=True)
        return False



# Delete S3 objects if transfer was successful and not in dry-run mode
if all_download_success and not dry_run:
    s3_prefix = f"temp/{timestamp}/{sanitize_service_name(service_name)}"
    if not delete_s3_prefix_objects(source_s3_client, s3_bucket, s3_prefix):
        logger.warning(f"‚ö†Ô∏è Failed to delete S3 objects for {service_name} at s3://{s3_bucket}/{s3_prefix}")
        service_result['message'] += '; S3 cleanup failed'
        service_result['status'] = 'Failed'






import boto3
import time
import logging
import os
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor
import json
from pathlib import Path
import uuid

'''Constants----------------------------------------------------------------------------'''
SPLIT_ON = 5 * 1024 * 1024 * 1024  # 5 GB
SPLIT_SIZE = 2 * 1024 * 1024 * 1024  # 2 GB
IS_SPLIT = False
MNT_PATH = '/mnt'
SSM_TIMEOUT = 1800
SSM_INTERVAL = 5
TAKE_BACKUP = True
BACKUP_PATH = '/mnt/Backup_timestamps/'
'''-------------------------------------------------------------------------------------'''

'''log setup----------------------------------------------------------------------------'''
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] [%(name)s] %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)
'''-------------------------------------------------------------------------------------'''

'''sanitize_service_name--------------------------------------------------------------------
Sanitize service name by replacing spaces and special characters with underscores.
Args:    service_name (str): The original service name.
Returns: str: The sanitized service name.
------------------------------------------------------------------------------------------'''
def sanitize_service_name(service_name):
    return service_name.replace(' ', '_').replace('/', '_').replace('\\', '_')

'''get_session------------------------------------------------------------------------------
Retrieve or create a boto3 session for the given profile name.
Args:    profile_name (str): The name of the AWS profile to use.
Returns: boto3.Session: A boto3 session associated with the specified profile.
------------------------------------------------------------------------------------------'''
session_cache = {}
size_tracker = {}
def get_session(profile_name):
    if profile_name not in session_cache:
        session_cache[profile_name] = boto3.Session(profile_name=profile_name)
    return session_cache[profile_name]

'''lambda_handler---------------------------------------------------------------------------
AWS Lambda function to process services listed in the event parameter.
Parameters: event (dict): Contains input data for the json:  context (object): Not used
Returns:    dict: Contains the overall status, message, and services results.
------------------------------------------------------------------------------------------'''
def lambda_handler(event, context):
    dry_run = event['dryRun']
    multi_threaded = True
    s3_bucket = event['s3Bucket']
    services = event.get("services", [])
    timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
    logger.info(f"üìú Starting execution | Multi-threaded: {multi_threaded} | Dry-run: {dry_run}")
    results = []
    overall_start_time = time.time()

    for service in services:
        service_name = service['name']
        if not service.get('enabled', False):
            logger.info(f"‚è≠Ô∏è Skipping disabled service: {service_name}")
            results.append({
                "enabled": False,
                "name": service_name,
                "source": service['source'],
                "destinations": service['destinations'],
                "status": "Skipped",
                "message": "Service disabled",
                "duration_seconds": 0
            })
            continue

        logger.info(f"üîÑ Processing service: {service_name}")
        start_time = time.time()
        service_result = {
            "enabled": True,
            "name": service_name,
            "source": service['source'],
            "destinations": service['destinations'],
            "status": "Pending",
            "upload_result": None,
            "download_results": []
        }
        source_env = service['source']['environment']
        source_session = get_session(source_env)
        source_ssm_client = source_session.client('ssm')
        source_s3_client = source_session.client('s3')
        source_instance = service['source']['instanceId']
        source_path = service['source']['path']

        upload_result = process_source_instance(
            source_ssm_client, source_s3_client,
            source_instance, source_path,
            s3_bucket, service_name, timestamp, dry_run
        )
        service_result["upload_result"] = upload_result

        if upload_result['status'] != 'Success':
            logger.error(f"‚ùå Upload failed for service {service_name}: {upload_result['message']}")
            service_result['status'] = 'Failed'
            service_result['message'] = 'Transfer failed'
            service_result['duration_seconds'] = upload_result.get('duration_seconds', 0)
            results.append(service_result)
            continue

        all_download_success = True

        def thread_wrapper(dest_ssm_client, dest_s3_client, dest_instance, s3_bucket, service_name, timestamp, dest_path, dry_run):
            logger.info(f"üì• [{dest_instance}] Starting download and extract for service {service_name}")
            result = process_destination_instance(
                dest_ssm_client, dest_s3_client,
                dest_instance, s3_bucket,
                service_name, timestamp, dest_path, dry_run
            )
            logger.info(f"‚úÖ [{dest_instance}] Download and extract completed with status: {result.get('status')}")
            return result

        if multi_threaded:
            logger.info(f"üîÑ Processing destinations in MULTI-THREADED mode for service: {service_name}")
            with ThreadPoolExecutor(max_workers=len(service['destinations'])) as executor:
                futures = []
                for destination in service['destinations']:
                    dest_env = destination['environment']
                    dest_session = get_session(dest_env)
                    dest_ssm_client = dest_session.client('ssm')
                    dest_s3_client = dest_session.client('s3')
                    dest_instance = destination['instanceId']
                    dest_path = destination['path']

                    logger.info(f"Submitting thread for destination {dest_instance}")
                    future = executor.submit(
                        thread_wrapper,
                        dest_ssm_client, dest_s3_client,
                        dest_instance, s3_bucket,
                        service_name, timestamp,
                        dest_path, dry_run
                    )
                    futures.append((future, destination))

                for future, destination in futures:
                    dest_instance = destination['instanceId']
                    try:
                        download_result = future.result()
                        download_result = {
                            'environment': destination['environment'],
                            'instanceId': destination['instanceId'],
                            'path': destination['path'],
                            'status': download_result['status'],
                            'service': download_result['service'],
                            'message': download_result['message'],
                            'duration_seconds': download_result['duration_seconds']
                        }
                        logger.info(f"Completed thread for {dest_instance} with status: {download_result['status']}")
                    except Exception as e:
                        logger.error(f"‚ùå Thread failed for {dest_instance}: {str(e)}", exc_info=True)
                        download_result = {
                            'environment': destination['environment'],
                            'instanceId': destination['instanceId'],
                            'path': destination['path'],
                            'status': 'Failed',
                            'service': service_name,
                            'message': f'Thread exception: {str(e)}',
                            'duration_seconds': 0
                        }
                    service_result['download_results'].append(download_result)
                    if download_result['status'] != 'Success':
                        all_download_success = False
        else:
            logger.info(f"üîÑ Processing destinations in SEQUENTIAL mode for service: {service_name}")
            for destination in service['destinations']:
                dest_env = destination['environment']
                dest_session = get_session(dest_env)
                dest_ssm_client = dest_session.client('ssm')
                dest_s3_client = dest_session.client('s3')

                dest_instance = destination['instanceId']
                dest_path = destination['path']

                logger.info(f"Starting sequential download for {dest_instance}")
                try:
                    download_result = process_destination_instance(
                        dest_ssm_client, dest_s3_client,
                        dest_instance, s3_bucket,
                        service_name, timestamp, dest_path, dry_run
                    )
                    download_result = {
                        'environment': destination['environment'],
                        'instanceId': destination['instanceId'],
                        'path': destination['path'],
                        'status': download_result['status'],
                        'service': download_result['service'],
                        'message': download_result['message'],
                        'duration_seconds': download_result['duration_seconds']
                    }
                except Exception as e:
                    logger.error(f"‚ùå Sequential error for {dest_instance}: {str(e)}", exc_info=True)
                    download_result = {
                        'environment': destination['environment'],
                        'instanceId': destination['instanceId'],
                        'path': destination['path'],
                        'status': 'Failed',
                        'service': service_name,
                        'message': f'Sequential exception: {str(e)}',
                        'duration_seconds': 0
                    }

                logger.info(f"‚úÖ Completed sequential download for {dest_instance} with status: {download_result['status']}")
                service_result['download_results'].append(download_result)
                if download_result['status'] != 'Success':
                    all_download_success = False

        service_result['status'] = 'Success' if all_download_success else 'Failed'
        service_result['message'] = 'Transfer completed' if all_download_success else 'Transfer failed'
        service_result['duration_seconds'] = time.time() - start_time
        results.append(service_result)

    logger.info("‚úÖ Execution completed in %.2f seconds", time.time() - overall_start_time)
    verify_sizes(size_tracker)

    return {
        "status": "Success",
        "message": f"Transfer finished at {timestamp}",
        "services": results
    }

def verify_sizes(size_dict):
    logger.info("üîç Verifying sizes of transferred data")
    sizes = list(size_dict.values())
    all_match = all(abs(sizes[0] - size) / sizes[0] < 0.01 for size in sizes) if sizes else True

    for instance, size in size_dict.items():
        logger.info(f"üìè {instance}: {size:,} bytes ({size/1024/1024:.2f} MB)")

    if all_match and sizes:
        logger.info("‚úÖ All database copies match within 1% tolerance")
    else:
        logger.warning("‚ö†Ô∏è Size mismatches detected")
        base_size = sizes[0] if sizes else 0
        for instance, size in size_dict.items():
            diff = size - base_size
            pct_diff = (diff / base_size) * 100 if base_size else 0
            logger.info(f"üìâ {instance} differs by {abs(diff):,} bytes ({abs(pct_diff):.2f}% from {base_size:,})")

'''service_via_ssm---------------------------------------------------------------------------
Executes specified action (start/stop) on a given service using AWS Systems Manager (SSM).
Parameters: ssm_client, instance_id, service, action
Returns:    bool: True if the command executed successfully, False otherwise.
------------------------------------------------------------------------------------------'''
def service_via_ssm(ssm_client, instance_id, service, action):
    commands = {
        'stop': {
            'Journey Engine': 'sudo -u engineadmin /opt/atpco/engine/scripts/maint_nl/eng-stop-journeyengine.sh',
            'Routings Engine': 'sudo -u engineadmin /opt/atpco/engine/scripts/maint_nl/eng-stop-routings.sh',
            'servicefee': 'sudo -u engineadmin /opt/atpco/engine/scripts/maint_nl/eng-stop-servicefee.sh',
            'farecheck': 'sudo -u engineadmin /opt/atpco/engine/scripts/maint_nl/eng-stop-farecheck.sh'
        },
        'start': {
            'Journey Engine': 'sudo -u engineadmin /opt/atpco/engine/scripts/maint_nl/eng-start-journeyengine.sh',
            'Routings Engine': 'sudo -u engineadmin /opt/atpco/engine/scripts/maint_nl/eng-start-routings.sh',
            'servicefee': 'sudo -u engineadmin /opt/atpco/engine/scripts/maint_nl/eng-start-servicefee.sh',
            'farecheck': 'sudo -u engineadmin /opt/atpco/engine/scripts/maint_nl/eng-start-farecheck.sh'
        }
    }
    command = commands[action].get(service)
    try:
        if command:
            start_time = time.time()
            status, output = run_ssm_command_and_get_multiple_output(ssm_client, instance_id, command)
            duration = time.time() - start_time
            logger.info(f"‚úÖ Service {service} {action} completed in {duration:.2f} seconds")
            logger.debug(f"Command output for {service} {action}: {output}")
            if status == 'Success':
                logger.info(f"‚úÖ {action.capitalize()} command for {service} executed successfully")
                return True
            else:
                logger.error(f"‚ùå Failed to execute {action} command for {service}. Status: {output}")
                return False
        else:
            logger.error(f"‚ùå No command found for {service} with action {action}")
            return False
    except Exception as e:
        logger.error(f"‚ùå Failed to {action} service {service} on instance {instance_id}: {str(e)}", exc_info=True)
        return False

'''get_available_size_via_ssm----------------------------------------------------------------
Retrieve the available size at the specified path on an instance using AWS Systems Manager (SSM).
Parameters: ssm_client, instance_id, source_path
Returns: int: The available size in bytes at the specified path. Returns 0 if an error occurs.
------------------------------------------------------------------------------------------'''
def get_available_size_via_ssm(ssm_client, instance_id, source_path):
    mkdir_cmd = f"mkdir -p {source_path}"
    try:
        run_ssm_command(ssm_client, instance_id, mkdir_cmd)
        cmd = f"df -k {source_path} | tail -1 | awk '{{print $4}}'"
        output = run_ssm_command_and_get_output(ssm_client, instance_id, cmd)
        size_in_bytes = int(output.strip()) * 1024
        logger.debug(f"Available size at {source_path} on {instance_id}: {size_in_bytes:,} bytes")
        return size_in_bytes
    except Exception as e:
        logger.error(f"‚ùå Failed to get available size for {source_path} on {instance_id}: {str(e)}", exc_info=True)
        return 0

'''process_source_instance-------------------------------------------------------------------
Processes the source instance, checking available disk space, compressing or splitting files,
uploading to S3, and restarting services using AWS Systems Manager (SSM).
Parameters: ssm_client, s3_client, instance_id, source_path, s3_bucket, service_folder, timestamp, dry_run
Returns: dict: Contains the status, service name, message, and duration of the upload process.
------------------------------------------------------------------------------------------'''
def process_source_instance(ssm_client, s3_client, instance_id, source_path, s3_bucket, service_folder, timestamp, dry_run):
    try:
        sanitized_service_name = sanitize_service_name(service_folder)
        logger.info(f"‚è≥ [{service_folder}] Starting source processing for instance {instance_id}")
        logger.debug(f"Source path: {source_path}, S3 bucket: {s3_bucket}, Timestamp: {timestamp}")
        
        start_time = time.time()
        dir_size = get_directory_size_via_ssm(ssm_client, instance_id, source_path)
        logger.info(f"‚úÖ [{service_folder}] Source directory size: {dir_size:,} bytes ({dir_size/1024/1024:.2f} MB)")
        size_tracker['Source_' + instance_id] = dir_size

        if dry_run:
            logger.info(f"[{service_folder}] Dry-run enabled. Skipping upload.")
            return {'status': 'Success', 'service': service_folder, 'message': 'Dry-run: Upload skipped', 'duration_seconds': 0}
        
        logger.info(f"[{service_folder}] Stopping service...")
        stop_status = service_via_ssm(ssm_client, instance_id, service_folder, 'stop')
        if not stop_status:
            logger.error(f"üõë [{service_folder}] Failed to stop service")
            return {'status': 'Failed', 'service': service_folder, 'message': f'Failed to stop: {service_folder} services', 'duration_seconds': time.time() - start_time}
        logger.info(f"üü¢ [{service_folder}] Service stopped successfully")

        global IS_SPLIT
        available_size_in_bytes = get_available_size_via_ssm(ssm_client, instance_id, source_path)
        if available_size_in_bytes > dir_size:
            if dir_size > SPLIT_ON:
                IS_SPLIT = True
                logger.info(f"[{service_folder}] Splitting directory due to size {dir_size:,} bytes > {SPLIT_ON:,} bytes")
                split_files = split_directory_via_ssm(ssm_client, instance_id, source_path, sanitized_service_name)
            else:
                IS_SPLIT = False
                logger.info(f"[{service_folder}] Compressing directory (size {dir_size:,} bytes <= {SPLIT_ON:,} bytes)")
                split_files = compress_directory_via_ssm(ssm_client, instance_id, source_path, sanitized_service_name)
        else:
            logger.warning(f"‚ö†Ô∏è Disk space issue - available: {available_size_in_bytes:,} bytes, required: {dir_size:,} bytes")
            difference_in_bytes = dir_size - available_size_in_bytes
            difference_in_mb = difference_in_bytes / (1024 * 1024)
            logger.error(f"‚ùå Insufficient disk space on {instance_id}. Need {difference_in_mb:.2f} MB more")
            return {'status': 'Failed', 'service': service_folder, 'message': f'Insufficient disk space: {difference_in_mb:.2f} MB needed', 'duration_seconds': time.time() - start_time}

        logger.info(f"[{service_folder}] Starting service...")
        start_status = service_via_ssm(ssm_client, instance_id, service_folder, 'start')
        if not start_status:
            logger.error(f"üõë [{service_folder}] Failed to start service")
            return {'status': 'Failed', 'service': service_folder, 'message': f'Failed to start: {service_folder} services', 'duration_seconds': time.time() - start_time}
        logger.info(f"üü¢ [{service_folder}] Service started successfully")

        if not split_files:
            logger.error(f"‚ùå [{service_folder}] No archive created")
            return {'status': 'Failed', 'service': service_folder, 'message': 'No archive created', 'duration_seconds': time.time() - start_time}

        s3_prefix = f"temp/{timestamp}/{sanitized_service_name}"
        
        logger.info(f"üì§ Uploading started to s3://{s3_bucket}/{s3_prefix}")
        upload_results = [upload_file_to_s3_via_ssm(ssm_client, instance_id, f, s3_bucket, s3_prefix) for f in split_files]

        cleanup_temp_files(ssm_client, instance_id, split_files)

        if not all(upload_results):
            logger.error(f"‚ùå [{service_folder}] Upload failed for one or more files")
            return {'status': 'Failed', 'service': service_folder, 'message': 'Upload failed', 'duration_seconds': time.time() - start_time}

        duration = time.time() - start_time
        logger.info(f"‚úÖ [{service_folder}] Upload completed in {duration:.2f} seconds")
        return {'status': 'Success', 'service': service_folder, 'message': 'Upload completed', 'duration_seconds': duration}

    except Exception as e:
        duration = time.time() - start_time
        if 'The SSO session associated with this profile has expired or is otherwise invalid' in str(e):
            logger.error(f"üîí SSO session expired for {service_folder}. Run: aws sso login --profile env")
            return {'status': 'Failed', 'service': service_folder, 'message': 'SSO login failed', 'duration_seconds': duration}
        else:
            logger.error(f"‚ùå Critical error in process_source_instance for {service_folder}: {str(e)}", exc_info=True)
            return {'status': 'Failed', 'service': service_folder, 'message': f"Error: {str(e)}", 'duration_seconds': duration}

'''process_destination_instance------------------------------------------------------------
Processes the destination instance by stopping services, checking available disk space, downloading and extracting files from S3,
and restarting services using AWS Systems Manager (SSM).
Parameters: ssm_client, s3_client, instance_id, s3_bucket, service_folder, timestamp, dest_path, dry_run
Returns: dict: Contains the status, service name, message, and duration of the download process.
------------------------------------------------------------------------------------------'''
def process_destination_instance(ssm_client, s3_client, instance_id, s3_bucket, service_folder, timestamp, dest_path, dry_run):
    try:
        sanitized_service_name = sanitize_service_name(service_folder)
        logger.info(f"‚è≥ [{service_folder}] Starting destination processing for instance {instance_id}")
        logger.debug(f"Destination path: {dest_path}, S3 bucket: {s3_bucket}, Timestamp: {timestamp}")
        start_time = time.time()
        if dry_run:
            logger.info(f"[{service_folder}] Dry-run enabled. Skipping download.")
            return {'status': 'Success', 'service': service_folder, 'message': 'Dry-run: Download skipped', 'duration_seconds': 0}

        logger.info(f"[{service_folder}] Stopping service...")
        stop_status = service_via_ssm(ssm_client, instance_id, service_folder, 'stop')
        if not stop_status:
            logger.error(f"üõë [{service_folder}] Failed to stop service")
            return {'status': 'Failed', 'service': service_folder, 'message': f'Failed to stop: {service_folder} services', 'duration_seconds': time.time() - start_time}
        logger.info(f"üü¢ [{service_folder}] Service stopped successfully")

        s3_prefix = f"temp/{timestamp}/{sanitized_service_name}"
        bucket_size = get_s3_bucket_size(s3_client, s3_bucket, s3_prefix)
        logger.info(f"[{service_folder}] S3 bucket size: {bucket_size:,} bytes ({bucket_size/1024/1024:.2f} MB)")

        available_size_in_bytes = get_available_size_via_ssm(ssm_client, instance_id, dest_path)
        if available_size_in_bytes <= bucket_size:
            logger.warning(f"‚ö†Ô∏è Disk space issue - available: {available_size_in_bytes:,} bytes, required: {bucket_size:,} bytes")
            difference_in_bytes = bucket_size - available_size_in_bytes
            difference_in_mb = difference_in_bytes / (1024 * 1024)
            logger.error(f"‚ùå Insufficient disk space on {instance_id}. Need {difference_in_mb:.2f} MB more")
            return {'status': 'Failed', 'service': service_folder, 'message': f'Insufficient disk space: {difference_in_mb:.2f} MB needed', 'duration_seconds': time.time() - start_time}

        if TAKE_BACKUP:
            logger.info(f"[{service_folder}] Creating backup of {dest_path} to {BACKUP_PATH}")
            backup_success, backup_message = backup_directory(ssm_client, instance_id, dest_path, service_folder, timestamp)
            if not backup_success:
                logger.error(f"‚ùå [{service_folder}] Backup failed: {backup_message}")
                return {'status': 'Failed', 'service': service_folder, 'message': f'Backup failed: {backup_message}', 'duration_seconds': time.time() - start_time}
            logger.info(f"‚úÖ [{service_folder}] Backup completed: {backup_message}")

        logger.info(f"[{service_folder}] Cleaning up destination directory {dest_path}")
        cleanup_directory(ssm_client, instance_id, dest_path)

        download_success, download_message = download_and_extract_files(ssm_client, instance_id, s3_bucket, s3_prefix, sanitized_service_name, dest_path)
        dir_size = get_directory_size_via_ssm(ssm_client, instance_id, dest_path)
        size_tracker['destination_' + instance_id] = dir_size
        logger.info(f"[{service_folder}] Destination directory size: {dir_size:,} bytes ({dir_size/1024/1024:.2f} MB)")

        logger.info(f"[{service_folder}] Starting service...")
        start_status = service_via_ssm(ssm_client, instance_id, service_folder, 'start')
        if not start_status:
            logger.error(f"üõë [{service_folder}] Failed to start service")
            return {'status': 'Failed', 'service': service_folder, 'message': f'Failed to start: {service_folder} services', 'duration_seconds': time.time() - start_time}
        logger.info(f"üü¢ [{service_folder}] Service started successfully")

        duration = time.time() - start_time
        if download_success:
            logger.info(f"‚úÖ [{service_folder}] Download and extraction completed in {duration:.2f} seconds")
            return {
                'status': 'Success',
                'service': service_folder,
                'message': f'Download and extraction completed for {service_folder}',
                'duration_seconds': duration
            }
        else:
            logger.error(f"‚ùå [{service_folder}] Download failed: {download_message}")
            return {
                'status': 'Failed',
                'service': service_folder,
                'message': f'Download failed: {download_message}',
                'duration_seconds': duration
            }

    except Exception as e:
        duration = time.time() - start_time
        logger.error(f"‚ùå Error processing destination for service {service_folder}: {str(e)}", exc_info=True)
        return {
            'status': 'Failed',
            'service': service_folder,
            'message': f'Error: {str(e)}',
            'duration_seconds': duration
        }

'''backup_directory-------------------------------------------------------------------------
Backs up the destination directory to the specified backup path with a timestamp.
Parameters: ssm_client, instance_id, dir_path, service_folder, timestamp
Returns: list: [success_bool, message]
------------------------------------------------------------------------------------------'''
def backup_directory(ssm_client, instance_id, dir_path, service_folder, timestamp):
    try:
        sanitized_service_name = sanitize_service_name(service_folder)
        backup_dir = f"{BACKUP_PATH}{timestamp}/{sanitized_service_name}"
        mkdir_cmd = f"mkdir -p {backup_dir}"
        run_ssm_command(ssm_client, instance_id, mkdir_cmd)
        logger.debug(f"Created backup directory {backup_dir} on {instance_id}")

        # Check if directory is empty
        check_cmd = f"[ -z \"$(ls -A {dir_path})\" ] && echo 'empty' || echo 'not_empty'"
        output = run_ssm_command_and_get_output(ssm_client, instance_id, check_cmd).strip()
        if output == 'empty':
            logger.info(f"[{service_folder}] Directory {dir_path} is empty, no backup needed")
            return [True, f"No backup needed for empty directory {dir_path}"]

        backup_cmd = f'cd {dir_path} && tar -cf - . | pigz -p 4 > {backup_dir}/{sanitized_service_name}.tar.gz'
        run_ssm_command(ssm_client, instance_id, backup_cmd)
        logger.info(f"‚úÖ [{service_folder}] Backed up {dir_path} to {backup_dir}/{sanitized_service_name}.tar.gz")
        return [True, f"Backed up {dir_path} to {backup_dir}/{sanitized_service_name}.tar.gz"]
    except Exception as e:
        logger.error(f"‚ùå Failed to backup {dir_path} for {service_folder}: {str(e)}", exc_info=True)
        return [False, f"Backup error: {str(e)}"]

'''run_ssm_command_and_get_output------------------------------------------------------------
Executes a shell command on an instance using AWS Systems Manager (SSM) and retrieves the output.
Parameters: ssm_client, instance_id, command
Returns: str: The standard output content of the executed command.
Raises: Exception: If the command execution fails or does not succeed.
------------------------------------------------------------------------------------------'''
def run_ssm_command_and_get_output(ssm_client, instance_id, command):
    try:
        logger.debug(f"Executing SSM command on {instance_id}: {command}")
        response = ssm_client.send_command(
            InstanceIds=[instance_id],
            DocumentName="AWS-RunShellScript",
            Parameters={'commands': [command]},
            TimeoutSeconds=SSM_TIMEOUT,
        )
        command_id = response['Command']['CommandId']
        for _ in range(int(SSM_TIMEOUT / SSM_INTERVAL)):
            time.sleep(SSM_INTERVAL)
            output = ssm_client.get_command_invocation(CommandId=command_id, InstanceId=instance_id)
            if output['Status'] in ['Success', 'Failed', 'TimedOut', 'Cancelled']:
                if output['Status'] != 'Success':
                    logger.error(f"‚ùå SSM command failed on {instance_id}: {output['StandardErrorContent']}")
                    raise Exception(output['StandardErrorContent'])
                logger.debug(f"SSM command succeeded on {instance_id}: {output['StandardOutputContent']}")
                return output['StandardOutputContent']
    except Exception as e:
        logger.error(f"‚ùå SSM command execution failed on {instance_id}: {str(e)}", exc_info=True)
        raise

'''run_ssm_command_and_get_multiple_output---------------------------------------------------
Executes a shell command on an instance using AWS Systems Manager (SSM) and retrieves the status and output.
Parameters: ssm_client, instance_id, command
Returns: tuple: Contains the status of the command execution and the standard output content.
Raises: Exception: If the command execution fails or does not succeed.
------------------------------------------------------------------------------------------'''
def run_ssm_command_and_get_multiple_output(ssm_client, instance_id, command):
    try:
        logger.debug(f"Executing SSM command on {instance_id}: {command}")
        response = ssm_client.send_command(
            InstanceIds=[instance_id],
            DocumentName="AWS-RunShellScript",
            Parameters={'commands': [command]},
            TimeoutSeconds=SSM_TIMEOUT,
        )
        command_id = response['Command']['CommandId']
        for _ in range(int(SSM_TIMEOUT / SSM_INTERVAL)):
            time.sleep(SSM_INTERVAL)
            output = ssm_client.get_command_invocation(CommandId=command_id, InstanceId=instance_id)
            if output['Status'] in ['Success', 'Failed', 'TimedOut', 'Cancelled']:
                if output['Status'] != 'Success':
                    logger.error(f"‚ùå SSM command failed on {instance_id}: {output['StandardErrorContent']}")
                    raise Exception(output['StandardErrorContent'])
                logger.debug(f"SSM command succeeded on {instance_id}")
                return output['Status'], output['StandardOutputContent']
    except Exception as e:
        logger.error(f"‚ùå SSM command execution failed on {instance_id}: {str(e)}", exc_info=True)
        raise

'''run_ssm_command---------------------------------------------------------------------------
Executes a shell command on an instance using AWS Systems Manager (SSM).
Parameters: ssm_client, instance_id, command
Returns: None
Raises: Exception: If the command execution fails or does not succeed.
------------------------------------------------------------------------------------------'''
def run_ssm_command(ssm_client, instance_id, command):
    try:
        logger.debug(f"Executing SSM command on {instance_id}: {command}")
        response = ssm_client.send_command(
            InstanceIds=[instance_id],
            DocumentName="AWS-RunShellScript",
            Parameters={'commands': [command]},
            TimeoutSeconds=SSM_TIMEOUT,
        )
        command_id = response['Command']['CommandId']
        for _ in range(int(SSM_TIMEOUT / SSM_INTERVAL)):
            time.sleep(SSM_INTERVAL)
            output = ssm_client.get_command_invocation(CommandId=command_id, InstanceId=instance_id)
            if output['Status'] in ['Success', 'Failed', 'TimedOut', 'Cancelled']:
                if output['Status'] != 'Success':
                    logger.error(f"‚ùå SSM command failed on {instance_id}: {output['StandardErrorContent']}")
                    raise Exception(output['StandardErrorContent'])
                logger.debug(f"SSM command succeeded on {instance_id}")
                return
    except Exception as e:
        logger.error(f"‚ùå SSM command execution failed on {instance_id}: {str(e)}", exc_info=True)
        raise

'''get_directory_size_via_ssm----------------------------------------------------------------
Retrieves the size of a directory on an instance using AWS Systems Manager (SSM).
Parameters: ssm_client, instance_id, source_path
Returns: int: The size of the directory in bytes.
------------------------------------------------------------------------------------------'''
def get_directory_size_via_ssm(ssm_client, instance_id, source_path):
    try:
        cmd = f"du -sb {source_path} | cut -f1"
        output = run_ssm_command_and_get_output(ssm_client, instance_id, cmd)
        size = int(output.strip())
        logger.debug(f"Directory size for {source_path} on {instance_id}: {size:,} bytes")
        return size
    except Exception as e:
        logger.error(f"‚ùå Failed to get directory size for {source_path} on {instance_id}: {str(e)}", exc_info=True)
        return 0

'''split_directory_via_ssm--------------------------------------------------------------------
Compresses and splits a directory into smaller parts on an instance using AWS Systems Manager (SSM).
Parameters: ssm_client, instance_id, source_path, zip_file_base
Returns: list: A list of paths to the split parts of the compressed archive.
------------------------------------------------------------------------------------------'''
def split_directory_via_ssm(ssm_client, instance_id, source_path, zip_file_base):
    sanitized_zip_file_base = sanitize_service_name(zip_file_base)
    tar_path = f"{MNT_PATH}/{sanitized_zip_file_base}.tar.gz"
    split_prefix = f"{MNT_PATH}/{sanitized_zip_file_base}.part."

    logger.info(f"[{zip_file_base}] Compressing and splitting directory {source_path}")
    cmd_compress = f'cd "{source_path}" && tar -cf - . | pigz -p 4 > "{tar_path}"'
    run_ssm_command(ssm_client, instance_id, cmd_compress)

    cmd_split = f'split -b "{SPLIT_SIZE}" -d -a 3 "{tar_path}" "{split_prefix}"'
    run_ssm_command(ssm_client, instance_id, cmd_split)

    list_cmd = f'ls "{split_prefix}*"'
    output = run_ssm_command_and_get_output(ssm_client, instance_id, list_cmd)
    split_files = output.strip().split('\n')
    logger.info(f"[{zip_file_base}] Split files created: {split_files}")
    return split_files

'''compress_directory_via_ssm-----------------------------------------------------------------
Compresses a directory into a tar.gz file on an instance using AWS Systems Manager (SSM).
Parameters: ssm_client, instance_id, source_path, zip_file_base
Returns: list: A list containing the path to the compressed archive.
------------------------------------------------------------------------------------------'''
def compress_directory_via_ssm(ssm_client, instance_id, source_path, zip_file_base):
    sanitized_zip_file_base = sanitize_service_name(zip_file_base)
    cmd = f'cd {source_path} && tar -cf - . | pigz -p 4 > "{MNT_PATH}/{sanitized_zip_file_base}.tar.gz"'
    run_ssm_command(ssm_client, instance_id, cmd)
    compressed_file = f"{MNT_PATH}/{sanitized_zip_file_base}.tar.gz"
    logger.info(f"[{zip_file_base}] Compressed directory to {compressed_file}")
    return [compressed_file]

'''upload_file_to_s3_via_ssm-----------------------------------------------------------------
Uploads a file to S3 using an SSM command.
Parameters: ssm_client, instance_id, file_path, s3_bucket, s3_prefix
Returns: bool: True if upload succeeds, False otherwise.
------------------------------------------------------------------------------------------'''
def upload_file_to_s3_via_ssm(ssm_client, instance_id, file_path, s3_bucket, s3_prefix):
    filename = os.path.basename(file_path)
    s3_key = f"{s3_prefix}/{filename}"
    cmd = f'aws s3 cp "{file_path}" "s3://{s3_bucket}/{s3_key}" --only-show-errors'
    try:
        run_ssm_command(ssm_client, instance_id, cmd)
        logger.info(f"‚úÖ Uploaded {filename} to s3://{s3_bucket}/{s3_key}")
        return True
    except Exception as e:
        logger.error(f"‚ùå Upload failed for {filename}: {str(e)}", exc_info=True)
        return False

'''cleanup_temp_files------------------------------------------------------------------------
Removes temporary files from the instance.
Parameters: ssm_client, instance_id, file_paths
Returns: None
------------------------------------------------------------------------------------------'''
def cleanup_temp_files(ssm_client, instance_id, file_paths):
    for file_path in file_paths:
        cmd = f'rm -f "{file_path}"'
        try:
            run_ssm_command(ssm_client, instance_id, cmd)
            logger.info(f"‚úÖ Cleaned up temporary file {file_path}")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Failed to clean up {file_path}: {str(e)}", exc_info=True)

'''cleanup_directory-------------------------------------------------------------------------
Cleans up the destination directory by removing all files.
Parameters: ssm_client, instance_id, dir_paths
Returns: None
------------------------------------------------------------------------------------------'''
def cleanup_directory(ssm_client, instance_id, dir_paths):
    cmd = f'rm -rf "{dir_paths}/*"'
    try:
        run_ssm_command(ssm_client, instance_id, cmd)
        logger.info(f"‚úÖ Successfully cleaned up directory {dir_paths}")
    except Exception as e:
        logger.error(f"‚ùå Failed to clean up {dir_paths}: {str(e)}", exc_info=True)

'''get_s3_bucket_size------------------------------------------------------------------------
Calculates the total size of objects in an S3 bucket under a specific prefix.
Parameters: s3_client, bucket_name, prefix
Returns: int: Total size in bytes.
------------------------------------------------------------------------------------------'''
def get_s3_bucket_size(s3_client, bucket_name, prefix=''):
    try:
        paginator = s3_client.get_paginator('list_objects_v2')
        total_size = 0
        for page in paginator.paginate(Bucket=bucket_name, Prefix=prefix):
            for obj in page.get('Contents', []):
                total_size += obj['Size']
        logger.debug(f"S3 bucket size for {bucket_name}/{prefix}: {total_size:,} bytes")
        return total_size
    except Exception as e:
        logger.error(f"‚ùå Failed to get S3 bucket size for {bucket_name}/{prefix}: {str(e)}", exc_info=True)
        return 0

'''download_and_extract_files----------------------------------------------------------------
Downloads and extracts files from S3 to the destination instance.
Parameters: ssm_client, instance_id, s3_bucket, s3_prefix, zip_file_base, dest_path
Returns: list: [success_bool, message]
------------------------------------------------------------------------------------------'''
def download_and_extract_files(ssm_client, instance_id, s3_bucket, s3_prefix, zip_file_base, dest_path):
    try:
        sanitized_zip_file_base = sanitize_service_name(zip_file_base)
        mkdir_cmd = f"mkdir -p {dest_path}"
        run_ssm_command(ssm_client, instance_id, mkdir_cmd)

        logger.info(f"üì• Downloading files from s3://{s3_bucket}/{s3_prefix}")
        download_cmd = f'aws s3 cp "s3://{s3_bucket}/{s3_prefix}/" "{MNT_PATH}/" --recursive --only-show-errors'
        run_ssm_command(ssm_client, instance_id, download_cmd)

        if IS_SPLIT:
            split_prefix = f"{MNT_PATH}/{sanitized_zip_file_base}.part."
            combined_file = f"{MNT_PATH}/{sanitized_zip_file_base}.tar.gz"
            list_parts_cmd = f"ls {split_prefix}* 2>/dev/null"
            parts_output = run_ssm_command_and_get_output(ssm_client, instance_id, list_parts_cmd)

            if parts_output.strip():
                logger.info(f"[{zip_file_base}] Joining split files: {parts_output.strip()}")
                join_cmd = f"ls {split_prefix}* | sort -V | xargs cat > {combined_file}"
                run_ssm_command(ssm_client, instance_id, join_cmd)
                cleanup_cmd = f'rm -f "{split_prefix}*"'
                run_ssm_command(ssm_client, instance_id, cleanup_cmd)
        else:
            combined_file = f"{MNT_PATH}/{sanitized_zip_file_base}.tar.gz"

        logger.info(f"[{zip_file_base}] Extracting files to {dest_path}")
        extract_cmd = f'cd {dest_path} && tar -xzf "{combined_file}"'
        run_ssm_command(ssm_client, instance_id, extract_cmd)

        rm_cmd = f'rm -f "{combined_file}"'
        run_ssm_command(ssm_client, instance_id, rm_cmd)

        logger.info(f"‚úÖ Successfully downloaded and extracted {zip_file_base} to {dest_path}")
        return [True, f"Download and extraction completed for {zip_file_base}"]

    except Exception as e:
        logger.error(f"‚ùå Failed to download/extract for {zip_file_base}: {str(e)}", exc_info=True)
        return [False, f"Error: {str(e)}"]

'''main--------------------------------------------------------------------------------------
Entry point for local execution, reading event from JSON file and saving output.
------------------------------------------------------------------------------------------'''
if __name__ == "__main__":
    script_dir = Path(__file__).resolve().parent
    event_path = script_dir / "event" / "event.json"
    with event_path.open() as f:
        test_event = json.load(f)
    logger.info(f"üìú Starting local execution with event: {json.dumps(test_event, indent=2)}")
    results = lambda_handler(test_event, None)
    logger.info(f"üü¢ Execution completed. Results summary: {json.dumps(results, indent=2)}")

    output_dir = Path("output")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"db_copy_output_{timestamp}.json"
    file_path = output_dir / filename
    
    with file_path.open("w") as f:
        json.dump(results, f, indent=2)
    logger.info(f"üìù Output saved to {file_path}")
