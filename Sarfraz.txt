import boto3
import pandas as pd
import io
import zipfile
import threading
from concurrent.futures import ThreadPoolExecutor
from tqdm import tqdm
from datetime import datetime
import re
import html
import configparser
import os
import webbrowser
import numpy as np
import logging

'''-----------------------------------
Setup Logging
------------------------------------'''
LOG_DIR = os.path.join(os.getcwd(), 'reports', 'log')
ts = datetime.now().strftime("%Y%m%d_%H%M%S")
LOG_FILE = os.path.join(LOG_DIR, f'log_{ts}.log')

try:
    os.makedirs(LOG_DIR, exist_ok=True)
    print(f"[✓] Directory ready: {LOG_DIR}")
except Exception as e:
    print(f"[✗] Error creating log directory: {e}")

try:
    logging.basicConfig(
        filename=LOG_FILE,
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )
    print(f"[✓] Logging configured to: {LOG_FILE}")
except Exception as e:
    print(f"[✗] Error configuring logging: {e}")

'''-----------------------------------
Configuration and Constants
------------------------------------'''
pd.set_option('mode.chained_assignment', None)

'''-----------------------------------
Reading config files
------------------------------------'''
config = configparser.ConfigParser()
config.read('config.ini')

# [settings]
project_name = config['settings']['project_name']
project_logo = config['settings']['project_logo']

# [report]
output_dir = config['report']['output_dir']
output_file = config['report']['output_file']
download_local = config.getboolean('download', 'download_local')

# [keys]
csv_primary_keys = config['keys']['primary_key_columns']
csv_columns = config['keys']['columns']

csv_primary_keys = [col.strip() for col in csv_primary_keys.split(',')] if csv_primary_keys else []
csv_columns = [col.strip() for col in csv_columns.split(',')] if csv_columns else None

# [aws]
bucket_name = config['aws']['bucket_name']
source_1_prefix = config['aws']['source_1_prefix']
source_2_prefix = config['aws']['source_2_prefix']

# [threading]
use_multithreading_reading = config.getboolean('threading', 'use_multithreading_reading')
use_multithreading_comparision = config.getboolean('threading', 'use_multithreading_comparision')

# [report_custom]
include_passed = config.getboolean('report_custom', 'include_passed')
include_missing_files = config.getboolean('report_custom', 'include_missing_files')
include_extra_files = config.getboolean('report_custom', 'include_extra_files')

global_percentage = config['global_col']['global_percentage']
global_percentage = [col.strip() for col in global_percentage.split(',')] if global_percentage else []

'''-----------------------------------
Utility Functions
------------------------------------'''
def create_dir(_dir_path):
    if _dir_path and not os.path.exists(_dir_path):
        os.makedirs(_dir_path)
        return True
    return True

def is_numeric(val):
    try:
        float(val)
        return True
    except (ValueError, TypeError):
        return False

def normalize_value(val):
    """Normalize values for comparison"""
    if pd.isna(val) or val is None or val == np.nan:
        return np.nan
    if isinstance(val, (int, float)):
        return float(val)
    if isinstance(val, str):
        val = val.strip()
        if val.lower() in ('null', 'none', 'nan', ''):
            return np.nan
        try:
            return float(val) if '.' in val else int(val)
        except ValueError:
            return val.lower()
    return val

def values_equal(val1, val2):
    """Enhanced value comparison with type normalization"""
    val1 = normalize_value(val1)
    val2 = normalize_value(val2)
    
    if pd.isna(val1) and pd.isna(val2):
        return True
    if pd.isna(val1) or pd.isna(val2):
        return False
    return val1 == val2

def normalize_filename(filename):
    return re.sub(r'\d{8}_\d{4}', '', os.path.basename(filename))

print_lock = threading.Lock()

def thread_safe_print(*args, **kwargs):
    with print_lock:
        logging.info(*args, **kwargs)

# S3 Functions
def get_s3_client(profile_name='p3-dev'):
    session = boto3.session.Session(profile_name=profile_name)
    return session.client('s3')

s3 = get_s3_client() if not download_local else None

def list_zip_files(prefix, download_local):
    if download_local:
        folder = os.path.join("downloads", prefix)
        if not os.path.exists(folder):
            raise FileNotFoundError(f"Local folder not found: {folder}")
        return [os.path.join(folder, f) for f in os.listdir(folder) if f.endswith('.zip')]
    else:
        try:
            response = s3.list_objects_v2(Bucket=bucket_name, Prefix=prefix)
            return [item['Key'] for item in response.get('Contents', []) if item['Key'].endswith('.zip')]
        except Exception as e:
            thread_safe_print(f"❌ {type(e).__name__}: {e}")
            return []

def read_zip_from_local(zip_path):
    csvs = {}
    try:
        with zipfile.ZipFile(zip_path, 'r') as z:
            for file_name in z.namelist():
                if file_name.endswith('.csv'):
                    with z.open(file_name) as f:
                        df = pd.read_csv(f)
                        csvs[file_name] = df
    except Exception as e:
        thread_safe_print(f"❌ Error reading local ZIP {zip_path}: {e}")
    return csvs

def read_zip_from_s3(zip_key):
    zip_obj = s3.get_object(Bucket=bucket_name, Key=zip_key)
    zip_data = zipfile.ZipFile(io.BytesIO(zip_obj['Body'].read()))
    csv_files = {}
    #for filename in zip_data.namelist():
    for filename in tqdm(zip_data.namelist(), desc="Processing Zip", unit="file", leave=True):
        if filename.endswith('.csv'):
            with zip_data.open(filename) as f:
                df = pd.read_csv(f, low_memory=False)
                csv_files[filename] = df
    return csv_files

def read_zip_from_s3_or_local(zip_key, download_local):
    return read_zip_from_local(zip_key) if download_local else read_zip_from_s3(zip_key)


from tqdm.contrib.concurrent import thread_map
import threading
import sys

'''-----------------------------------
read_all_csvs_by_source
------------------------------------'''
def read_all_csvs_by_source(zip_keys, source_name, download_local, use_multithreading=True):
    all_csvs = {}
    store_lock = threading.Lock()

    logging.info(f"Number of ZIPs for {source_name}: {len(zip_keys)}")  # Debug

    def read_zip_and_store(zip_key):
        try:
            csvs = read_zip_from_s3_or_local(zip_key, download_local)
            for name, df in csvs.items():
                with store_lock:
                    if name not in all_csvs:
                        all_csvs[name] = df
                    else:
                        thread_safe_print(f"⚠️ Duplicate CSV: {name} from {zip_key}")
        except Exception as e:
            thread_safe_print(f"❌ Failed to read {zip_key}: {e}")

    if use_multithreading:
        thread_map(read_zip_and_store, zip_keys, total=len(zip_keys),
                   desc=f"Reading ZIPs from {source_name}", unit="zip",
                   max_workers=8, file=sys.stdout, dynamic_ncols=True)
    else:
        for zip_key in tqdm(zip_keys, desc=f"Reading ZIPs from {source_name}", unit="zip",
                            file=sys.stdout, dynamic_ncols=True):
            read_zip_and_store(zip_key)

    return all_csvs

'''-----------------------------------
Comparison Functions
------------------------------------'''
def compare_csvs(df1, df2, file_name):
    """Enhanced CSV comparison with detailed discrepancy tracking"""
    summary = {
        'Missing Columns in Neoprice': [],
        'Missing Columns in Engine': [],
        'Missing Rows in Neoprice': 0,
        'Extra Rows in Neoprice': 0,
        'Duplicate Rows in Engine': 0,
        'Duplicate Rows in Neoprice': 0,
        'Total Fields Compared': 0,
        'Number of Discrepancies': 0,
        'Number of Row Discrepancies': 0,  # ✅ New metric
        'Field Mismatches': 0,
        'Failure %': 0.0,
        'Pass %': 0.0,
        'Row Failure %': 0.0,  # ✅ New metric
        'Row Pass %': 0.0,     # ✅ New metric
        'Status': 'PASS',
        'Total Rows in Engine': 0,
        'Total Rows in Neoprice': 0,
        'Global Percentage Metrics': {col: {'Matches': 0, 'Mismatches': 0, 'Total': 0} for col in global_percentage}
    }
    diff_summary = []

    # Select only specified columns if provided
    if csv_columns:
        df1 = df1[csv_columns]
        df2 = df2[csv_columns]

    # Track column differences
    summary['Missing Columns in Neoprice'] = list(set(df1.columns) - set(df2.columns))
    summary['Missing Columns in Engine'] = list(set(df2.columns) - set(df1.columns))

    common_columns = list(set(df1.columns).intersection(set(df2.columns)))
    if not common_columns:
        logging.info(f"No common columns to compare in {file_name}")
        return pd.DataFrame(), summary

    # Reset indices and track original row numbers
    df1 = df1.reset_index(drop=True)
    df2 = df2.reset_index(drop=True)
    df1['_original_row'] = df1.index + 1
    df2['_original_row'] = df2.index + 1

    # Clean primary keys
    for key in csv_primary_keys:
        df1[key] = df1[key].astype(str).str.strip()
        df2[key] = df2[key].astype(str).str.strip()

    # Convert relevant columns to categorical where appropriate
    for col in csv_primary_keys:
        if len(df1) > 0 and df1[col].nunique() / len(df1) < 0.5:
            df1[col] = df1[col].astype('category')
            df2[col] = df2[col].astype('category')

    # Remove rows with null primary keys
    df1 = df1.dropna(subset=csv_primary_keys)
    df2 = df2.dropna(subset=csv_primary_keys)

    # Add total cleaned row counts to summary
    summary['Total Rows in Engine'] = len(df1)
    summary['Total Rows in Neoprice'] = len(df2)

    # Set primary keys as index and sort
    df1 = df1.set_index(csv_primary_keys).sort_index()
    df2 = df2.set_index(csv_primary_keys).sort_index()

    # Track duplicates
    summary['Duplicate Rows in Engine'] = df1.index.duplicated().sum()
    summary['Duplicate Rows in Neoprice'] = df2.index.duplicated().sum()

    dup_rows_engine = df1[df1.index.duplicated(keep=False)]
    dup_rows_neoprice = df2[df2.index.duplicated(keep=False)]

    for pk in dup_rows_engine.index.unique():
        row_numbers = dup_rows_engine.loc[pk]['_original_row'].tolist()
        diff_summary.append({
            'PrimaryKey': pk,
            'Column': 'DUPLICATE_ROW',
            'Engine_Value': f"Duplicate ({len(row_numbers)} occurrences)",
            'Neoprice_Value': '',
            'RowNum_Engine': ', '.join(map(str, row_numbers)),
            'RowNum_Neoprice': '',
            'Status': 'Duplicate in Engine'
        })

    for pk in dup_rows_neoprice.index.unique():
        row_numbers = dup_rows_neoprice.loc[pk]['_original_row'].tolist()
        diff_summary.append({
            'PrimaryKey': pk,
            'Column': 'DUPLICATE_ROW',
            'Engine_Value': '',
            'Neoprice_Value': f"Duplicate ({len(row_numbers)} occurrences)",
            'RowNum_Engine': '',
            'RowNum_Neoprice': ', '.join(map(str, row_numbers)),
            'Status': 'Duplicate in Neoprice'
        })

    # Remove duplicates for further comparison
    df1 = df1[~df1.index.duplicated()]
    df2 = df2[~df2.index.duplicated()]

    # Track missing and extra rows
    missing_in_neoprice = df1.index.difference(df2.index)
    extra_in_neoprice = df2.index.difference(df1.index)

    summary['Missing Rows in Neoprice'] = len(missing_in_neoprice)
    summary['Extra Rows in Neoprice'] = len(extra_in_neoprice)

    for pk in missing_in_neoprice:
        diff_summary.append({
            'PrimaryKey': pk,
            'Column': 'MISSING_ROW',
            'Engine_Value': 'Exists',
            'Neoprice_Value': 'Missing',
            'RowNum_Engine': df1.loc[pk, '_original_row'],
            'RowNum_Neoprice': '',
            'Status': 'Missing in Neoprice'
        })

    for pk in extra_in_neoprice:
        diff_summary.append({
            'PrimaryKey': pk,
            'Column': 'EXTRA_ROW',
            'Engine_Value': 'Missing',
            'Neoprice_Value': 'Exists',
            'RowNum_Engine': '',
            'RowNum_Neoprice': df2.loc[pk, '_original_row'],
            'Status': 'Extra in Neoprice'
        })

    # Compare common rows
    common_idx = df1.index.intersection(df2.index)
    total_fields = 0
    mismatches = 0
    discrepant_rows = set()  # Track rows with discrepancies

    for idx in tqdm(common_idx, desc=f"Comparing rows ({file_name})", unit="rows", dynamic_ncols=True, leave=True):
        if isinstance(df1.index, pd.MultiIndex):
            row1 = df1.loc[tuple(idx)] if isinstance(idx, (list, tuple)) else df1.loc[idx]
            row2 = df2.loc[tuple(idx)] if isinstance(idx, (list, tuple)) else df2.loc[idx]
        else:
            row1 = df1.loc[idx]
            row2 = df2.loc[idx]

        row1_number = int(row1['_original_row'])
        row2_number = int(row2['_original_row'])

        row_has_mismatch = False
        for col in common_columns:
            val1 = row1.get(col, None)
            val2 = row2.get(col, None)
            total_fields += 1

            if col in global_percentage:
                summary['Global Percentage Metrics'][col]['Total'] += 1
                if values_equal(val1, val2):
                    summary['Global Percentage Metrics'][col]['Matches'] += 1
                else:
                    summary['Global Percentage Metrics'][col]['Mismatches'] += 1

            if not values_equal(val1, val2):
                mismatches += 1
                row_has_mismatch = True
                diff_summary.append({
                    'PrimaryKey': idx,
                    'Column': col,
                    'Engine_Value': val1,
                    'Neoprice_Value': val2,
                    'RowNum_Engine': row1_number,
                    'RowNum_Neoprice': row2_number,
                    'Status': 'Mismatch'
                })

        if row_has_mismatch:
            discrepant_rows.add(idx)

    # Add rows with missing, extra, or duplicate issues to discrepant_rows
    discrepant_rows.update(missing_in_neoprice)
    discrepant_rows.update(extra_in_neoprice)
    discrepant_rows.update(dup_rows_engine.index)
    discrepant_rows.update(dup_rows_neoprice.index)

    summary['Total Fields Compared'] = total_fields
    summary['Field Mismatches'] = mismatches
    summary['Number of Row Discrepancies'] = len(discrepant_rows)  # ✅ Count unique discrepant rows

    missing_rows = len(missing_in_neoprice)
    extra_rows = len(extra_in_neoprice)
    duplicates = summary['Duplicate Rows in Engine'] + summary['Duplicate Rows in Neoprice']
    field_mismatches = mismatches

    total_discrepancies = missing_rows + extra_rows + duplicates + field_mismatches
    summary['Number of Discrepancies'] = total_discrepancies

    total_data_points = len(df1) + len(df2) + total_discrepancies

    if total_data_points > 0:
        failure_percent = (total_discrepancies / total_data_points) * 100
        if failure_percent == 0 and total_discrepancies > 0:
            failure_percent = 0.000001
        summary['Failure %'] = round(failure_percent, 6)
        summary['Pass %'] = round(100 - summary['Failure %'], 6)
    else:
        summary['Failure %'] = 0.0
        summary['Pass %'] = 100.0

    # Calculate Row Failure % and Row Pass %
    total_engine_rows = summary['Total Rows in Engine']
    if total_engine_rows > 0:
        row_failure_percent = (summary['Number of Row Discrepancies'] / total_engine_rows) * 100
        summary['Row Failure %'] = round(row_failure_percent, 6)
        summary['Row Pass %'] = round(100 - row_failure_percent, 6)
    else:
        summary['Row Failure %'] = 0.0
        summary['Row Pass %'] = 100.0

    if total_discrepancies > 0:
        summary['Status'] = 'FAIL'
        summary['Note'] = f'❌ Found {total_discrepancies} discrepancies'
    else:
        summary['Status'] = 'PASS'
        summary['Note'] = '✅ No comparison issues, files are identical'

    diff_df = pd.DataFrame(diff_summary)
    return diff_df, summary

'''-----------------------------------
compare_all_csvs
------------------------------------'''
def compare_all_csvs(all_csvs_source1, all_csvs_source2, use_multithreading=True):
    normalized_source1 = {normalize_filename(k): k for k in all_csvs_source1}
    normalized_source2 = {normalize_filename(k): k for k in all_csvs_source2}

    common_csvs = set(normalized_source1) & set(normalized_source2)
    missing_in_source2 = set(normalized_source1) - set(normalized_source2)
    missing_in_source1 = set(normalized_source2) - set(normalized_source1)

    all_diffs = []
    all_summaries = {}
    global_metrics = {col: {'Matches': 0, 'Mismatches': 0, 'Total': 0} for col in global_percentage}

    def process_csv_pair(csv_name):
        df1 = all_csvs_source1[normalized_source1[csv_name]]
        df2 = all_csvs_source2[normalized_source2[csv_name]]
        return csv_name, compare_csvs(df1, df2, csv_name)

    if use_multithreading:
        with ThreadPoolExecutor(max_workers=64) as executor:
            results = list(executor.map(process_csv_pair, common_csvs))
    else:
        results = [process_csv_pair(name) for name in common_csvs]

    for csv_name, (diff_df, summary) in results:
        if diff_df.empty:
            summary['Note'] = '✅ No differences'
        else:
            diff_df['File'] = csv_name
            all_diffs.append(diff_df)
        all_summaries[csv_name] = summary

        if 'Global Percentage Metrics' in summary:
            for col in global_percentage:
                if col in summary['Global Percentage Metrics']:
                    global_metrics[col]['Matches'] += summary['Global Percentage Metrics'][col]['Matches']
                    global_metrics[col]['Mismatches'] += summary['Global Percentage Metrics'][col]['Mismatches']
                    global_metrics[col]['Total'] += summary['Global Percentage Metrics'][col]['Total']

    global_percentages = {}
    for col in global_percentage:
        total = global_metrics[col]['Total']
        matches = global_metrics[col]['Matches']
        mismatches = global_metrics[col]['Mismatches']
        if total > 0:
            pass_percentage = (matches / total) * 100
            global_percentages[col] = {
                'Pass %': round(pass_percentage, 2),
                'Fail Count': mismatches,
                'Total Rows': total
            }
        else:
            global_percentages[col] = {
                'Pass %': 100.0,
                'Fail Count': 0,
                'Total Rows': 0
            }

    all_summaries['Global Percentages'] = global_percentages

    if missing_in_source2:
        all_summaries["Missing in Source2"] = list(missing_in_source2)
    if missing_in_source1:
        all_summaries["Extra in Source2"] = list(missing_in_source1)

    final_diff_df = pd.concat(all_diffs) if all_diffs else pd.DataFrame()
    return final_diff_df, all_summaries

import html
import logging
import pandas as pd
import numpy as np
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor
from tqdm import tqdm

'''-----------------------------------
generate_html_report
------------------------------------'''
def generate_html_report(
    diff_df, 
    summary, 
    report_start_time, 
    output_file,
    source_files_count, 
    destination_files_count, 
    primary_key_columns=None,  # Assuming csv_primary_keys is a list
    columns=None,              # Assuming csv_columns is a list
    project_name="Project",
    project_logo="logo.png",
    include_passed=True,
    include_missing_files=True,
    include_extra_files=True,
    global_percentage=None,
    use_multithreading=False   # New flag for multithreading
):
    report_end_time = datetime.now()
    time_taken = report_end_time - report_start_time
    time_taken_str = str(time_taken).split('.')[0]  # HH:MM:SS

    comparison_rows = ""
    
    # Initialize metrics
    total_fields_compared = 0
    total_row_discrepancies = 0
    total_missing_files = 0
    total_missing_rows = 0
    total_extra_rows = 0
    total_duplicates = 0
    total_engine_rows = 0
    total_neoprice_rows = 0
    overall_row_pass = 0.0

    # Thread-safe storage for global percentage metrics
    global_percentage_metrics = {}
    if global_percentage:
        for col in global_percentage:
            global_percentage_metrics[col] = {
                'total': 0,
                'mismatches': 0,
                'pass_percent': 100.0,
                'fail_percent': 0.0
            }

    def format_value(val):
        if pd.isna(val) or val is None or val == np.nan:
            return ""
        try:
            if isinstance(val, (int, float)):
                if float(val).is_integer():
                    return str(int(val))
                return "{:.4f}".format(val).rstrip('0').rstrip('.')
        except (ValueError, TypeError):
            pass
        return str(val)

    def calculate_diff(val1, val2):
        try:
            num1 = float(val1)
            num2 = float(val2)
            diff = num1 - num2
            if abs(diff) < 1e-10:
                diff = 0.0
            if diff.is_integer():
                return str(int(diff))
            return "{:.4f}".format(diff).rstrip('0').rstrip('.')
        except (ValueError, TypeError):
            return "N/A"

    # Validate required columns
    required_columns = ['PrimaryKey', 'Status', 'RowNum_Engine', 'RowNum_Neoprice',
                       'Engine_Value', 'Neoprice_Value', 'Column']
    missing_cols = [col for col in required_columns if col not in diff_df.columns]
    if missing_cols:
        raise ValueError(f"Missing required columns in diff_df: {missing_cols}")

    # First pass to calculate all metrics
    for csv_file, file_summary in tqdm(summary.items(), desc="Calculating metrics"):
        if csv_file in ["Missing in Source2", "Extra in Source2"]:
            continue
        if not isinstance(file_summary, dict):
            continue

        total_row_discrepancies += file_summary.get('Number of Row Discrepancies', 0)
        engine_rows = file_summary.get('Total Rows in Engine', 0)
        neoprice_rows = file_summary.get('Total Rows in Neoprice', 0)
        total_engine_rows += engine_rows
        total_neoprice_rows += neoprice_rows

        fields = file_summary.get('Total Fields Compared', 0)
        if fields:
            total_fields_compared += fields
            total_duplicates += file_summary.get('Duplicate Rows in Engine', 0) + file_summary.get('Duplicate Rows in Neoprice', 0)
            total_missing_rows += file_summary.get('Missing Rows in Neoprice', 0)
            total_extra_rows += file_summary.get('Extra Rows in Neoprice', 0)

    # Calculate overall row pass rate
    if total_engine_rows > 0:
        overall_row_pass = ((total_engine_rows - total_row_discrepancies) / total_engine_rows) * 100
        overall_row_pass = round(overall_row_pass, 5)
    else:
        overall_row_pass = 100.0

    def process_file(csv_file, file_summary):
        """Process a single CSV file and return its HTML comparison row."""
        if csv_file in ["Missing in Source2", "Extra in Source2"]:
            return None
        if not isinstance(file_summary, dict):
            return None

        file_diff_df = diff_df[diff_df['File'] == csv_file] if not diff_df.empty else pd.DataFrame()
        match_status = file_summary.get('Status', 'PASS')

        if match_status == "PASS" and not include_passed:
            return None

        icon = "<i class='fas fa-check-circle' style='color:green;'></i>" if match_status == "PASS" else "<i class='fas fa-times-circle' style='color: red;'></i>"

        # Global percentage calculations (thread-safe)
        local_metrics = {}
        if global_percentage and not file_diff_df.empty:
            logging.info(f"Processing file: {csv_file}, diff_df rows: {len(file_diff_df)}")
            for col in global_percentage:
                local_metrics[col] = {'mismatches': 0, 'total': 0}
                col_mismatches = file_diff_df[(file_diff_df['Column'] == col) & 
                                            (file_diff_df['Status'] == 'Mismatch')].shape[0]
                total_comparisons = file_diff_df[file_diff_df['Column'] == col].shape[0]
                local_metrics[col]['mismatches'] = col_mismatches
                local_metrics[col]['total'] = total_comparisons

        # Group discrepancies by primary key
        diff_groups = {}
        if not file_diff_df.empty:
            for _, row in file_diff_df.iterrows():
                key = (row['PrimaryKey'], row['Status'])
                if key not in diff_groups:
                    diff_groups[key] = {
                        'RowNum_Engine': row['RowNum_Engine'],
                        'RowNum_Neoprice': row['RowNum_Neoprice'],
                        'details': []
                    }
                diff_groups[key]['details'].append(row)

        # Build difference table with tqdm
        diff_table_rows = []
        for (primary_key, status), group in tqdm(diff_groups.items(), desc=f"calculating diffrences for  {csv_file}", unit="file", leave=True):
            rowspan = len(group['details'])
            cl_primary_key = tuple(x if x != 'nan' else '' for x in primary_key)
            
            header_row = f"""
            <tr>
                <td rowspan="{rowspan}" style="vertical-align: top;">
                    <small>{html.escape(str(cl_primary_key))}</small><br>
                    <small>Engine Row: {group['RowNum_Engine'] or '-'} Neoprice Row: {group['RowNum_Neoprice'] or '-'}</small>
                </td>
            """
            
            first_detail = group['details'][0]
            diff_value = calculate_diff(first_detail['Engine_Value'], first_detail['Neoprice_Value'])
            diff_class = "positive-diff" if diff_value != "N/A" and float(diff_value) > 0 else "negative-diff" if diff_value != "N/A" and float(diff_value) < 0 else ""
            
            header_row += f"""
                <td><small>{html.escape(str(first_detail['Column']))}</small></td>
                <td><small>{html.escape(format_value(first_detail['Engine_Value']))}</small></td>
                <td><small>{html.escape(format_value(first_detail['Neoprice_Value']))}</small></td>
                <td class="numeric-diff {diff_class}"><small>{diff_value}</small></td>
                <td rowspan="{rowspan}" style="vertical-align: middle;"><small>{status}</small></td>
            </tr>
            """
            diff_table_rows.append(header_row)
            
            for detail in group['details'][1:]:
                diff_value = calculate_diff(detail['Engine_Value'], detail['Neoprice_Value'])
                diff_class = "positive-diff" if diff_value != "N/A" and float(diff_value) > 0 else "negative-diff" if diff_value != "N/A" and float(diff_value) < 0 else ""
                
                diff_table_rows.append(f"""
                <tr>
                    <td><small>{html.escape(str(detail['Column']))}</small></td>
                    <td><small>{html.escape(format_value(detail['Engine_Value']))}</small></td>
                    <td><small>{html.escape(format_value(detail['Neoprice_Value']))}</small></td>
                    <td class="numeric-diff {diff_class}">{diff_value}</td>
                </tr>
                """)

        diff_table = "".join(diff_table_rows)

        # Build mismatch details section
        if match_status == "FAIL":
            xrow_disc = file_summary.get('Number of Row Discrepancies', 0) - (
                file_summary.get('Missing Rows in Neoprice', 0) +
                file_summary.get('Extra Rows in Neoprice', 0) +
                (file_summary.get('Duplicate Rows in Engine', 0) + file_summary.get('Duplicate Rows in Neoprice', 0))
            )
            mismatch_details = f"""
                <div>
                    <button class="toggle-button" onclick="toggleVisibility('diff-{csv_file}', this)">+</button>
                    <span class="summary-badge" style="background-color: #f8d7da; color: #721c24; padding: 2px 6px; border-radius: 4px; margin-left: 5px;">
                        {file_summary.get('Number of Row Discrepancies', 0)} discrepancies
                        {f"| {file_summary.get('Row Failure %', 0.0):.6f}% failure" if file_summary.get('Number of Discrepancies', 0) > 0 else ""}
                        {f"| row discrepancies:{xrow_disc}" if xrow_disc > 0 else ""}
                        {f"| missing rows:{file_summary.get('Missing Rows in Neoprice', 0)}" if file_summary.get('Missing Rows in Neoprice', 0) > 0 else ""}
                        {f"| extra rows:{file_summary.get('Extra Rows in Neoprice', 0)}" if file_summary.get('Extra Rows in Neoprice', 0) > 0 else ""}
                        {f"| duplicate rows:{file_summary.get('Duplicate Rows in Engine', 0) + file_summary.get('Duplicate Rows in Neoprice', 0)}" if (file_summary.get('Duplicate Rows in Engine', 0) + file_summary.get('Duplicate Rows in Neoprice', 0)) > 0 else ""}
                    </span>
                    <div id="diff-{csv_file}" style="display:none; margin-top: 10px;">
                        <table class="diff-table">
                            <thead>
                                <tr>
                                    <th width="50%">Primary Key</th>
                                    <th width="10%">Column</th>
                                    <th width="10%">Engine</th>
                                    <th width="10%">Neoprice</th>
                                    <th width="10%">Diff</th>
                                    <th width="10%">Status</th>
                                </tr>
                            </thead>
                            <tbody>
                                {diff_table}
                            </tbody>
                        </table>
                    </div>
                </div>
            """
        else:
            mismatch_details = """
                <div style="color: green;">
                    The files are identical. No differences were found during the comparison.
                </div>
            """

        comparison_row = f"""
        <tr>
            <td><i class="fas fa-file-csv" style="color:blue;"></i> {csv_file}</td>
            <td align="center">{icon}</td>
            <td>{mismatch_details}</td>
        </tr>
        """
        return comparison_row, local_metrics

    # Process files: multithreading or single-threaded
    comparison_rows_list = []
    if use_multithreading:
        with ThreadPoolExecutor() as executor:
            futures = [
                executor.submit(process_file, csv_file, file_summary)
                for csv_file, file_summary in summary.items()
            ]
            for future in tqdm(futures, desc="Generating Report"):
                result = future.result()
                if result:
                    comparison_row, local_metrics = result
                    if comparison_row:
                        comparison_rows_list.append(comparison_row)
                    # Update global metrics thread-safely
                    for col, metrics in local_metrics.items():
                        global_percentage_metrics[col]['total'] += metrics['total']
                        global_percentage_metrics[col]['mismatches'] += metrics['mismatches']
    else:
        for csv_file, file_summary in tqdm(summary.items(), desc="Generating Report"):
            result = process_file(csv_file, file_summary)
            if result:
                comparison_row, local_metrics = result
                if comparison_row:
                    comparison_rows_list.append(comparison_row)
                # Update global metrics
                for col, metrics in local_metrics.items():
                    global_percentage_metrics[col]['total'] += metrics['total']
                    global_percentage_metrics[col]['mismatches'] += metrics['mismatches']

    global_percentages = summary.get('Global Percentages', {})
    global_metrics = [
        {
            'column': html.escape(col),
            'pass_percentage': metrics['Pass %'],
            'fail_count': metrics['Fail Count'],
            'total_rows': metrics['Total Rows'],
            'metric_class': "pass-metric" if metrics['Pass %'] >= 99.99 else "fail-metric" if metrics['Pass %'] < 90 else "warn-metric"
        }
        for col, metrics in global_percentages.items()
    ]

    # Combine comparison rows
    comparison_rows = "".join(comparison_rows_list)

    # Add missing/extra files
    if include_missing_files and "Missing in Source2" in summary:
        total_missing_files = len(summary["Missing in Source2"])
        for missing_csv in summary["Missing in Source2"]:
            comparison_rows += f"""
            <tr>
                <td><i class="fas fa-file-csv" style="color:gray;"></i> {missing_csv}</td>
                <td align="center"><i class="fas fa-exclamation-triangle" style="color:orange;"></i></td>
                <td>Missing in Neoprice</td>
            </tr>
            """
    if include_extra_files and "Extra in Source2" in summary:
        for extra_csv in summary["Extra in Source2"]:
            comparison_rows += f"""
            <tr>
                <td><i class="fas fa-file-csv" style="color:gray;"></i> {extra_csv}</td>
                <td align="center"><i class="fas fa-exclamation-triangle" style="color:blue;"></i></td>
                <td>Extra in Neoprice</td>
            </tr>
            """

    # Build global percentage section
    # Build global percentage section
    global_percentage_section = ""
    if global_percentage:
        global_percentage_section = "<h2>📈 Column-Specific Metrics</h2>"
        global_percentage_section += "<div class='metrics-container'>"
        for metric in global_metrics:
            global_percentage_section += f"""
                <div class="metric-card {metric['metric_class']}">
                    <div class="metric-value">{metric['pass_percentage']}%</div>
                    <div class="metric-label">{metric['column']}</div>
                    <div class="metric-subtext">{metric['fail_count']} fail out of {metric['total_rows']} rows</div>
                </div>
            """
        global_percentage_section += "</div>"

    # HTML template (unchanged)
    html_template = f"""<!DOCTYPE html>
    <html>
    <head>
        <meta charset="UTF-8">
        <title>{project_name} Report</title>
        <link rel="icon" type="image/x-icon" href="{project_logo}">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
        <style>
            body {{ font-family: Arial, sans-serif; background: #f4f4f9; margin: 0; padding: 0; }}
            .container {{ background: white; margin: 10px auto; padding: 10px; max-width: 98%; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.1); }}
            header {{ background-color: #173E72; color: white; padding: 5px; text-align: center; }}
            table {{ width: 100%; border-collapse: collapse; margin-top: 15px; word-break: break-word; table-layout: fixed; }}
            th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}
            th {{ background: #173E72; color: white; position: sticky; top: 0; }}
            tr:nth-child(even) {{ background: #f9f9f9; }}
            tr:hover {{ background: #f1f1f1; }}
            .toggle-button {{ font-size: 0.8em; padding: 3px 8px; background-color: #0056b3; color: white; border: none; border-radius: 4px; cursor: pointer; }}
            .summary-badge {{ font-weight: bold; }}
            .diff-table th {{ background: #173E72; }}
            .summary-grid {{
                display: grid;
                grid-template-columns: repeat(3, 1fr);
                gap: 10px;
                margin-top: 15px;
            }}
            .summary-item {{
                background: #f8f9fa;
                padding: 8px;
                border-radius: 4px;
                border-left: 4px solid #173E72;
            }}
            .summary-label {{
                font-weight: bold;
                color: #495057;
            }}
            .summary-value {{
                color: #212529;
                margin-left: 5px;
            }}
            .numeric-diff {{
                font-family: monospace;
                text-align: right;
            }}
            .positive-diff {{ color: #d9534f; }}
            .negative-diff {{ color: #5cb85c; }}
            header img {{
                height: 60px;
                width: auto;
                max-width: 200px;
            }}
            .metrics-container {{
                display: grid;
                grid-template-columns: repeat(auto-fill, minmax(200px, 1fr));
                gap: 15px;
                margin: 20px 0;
            }}
            .metric-card {{
                background: white;
                border-radius: 8px;
                padding: 15px;
                box-shadow: 0 2px 4px rgba(0,0,0,0.1);
                text-align: center;
            }}
            .metric-value {{
                font-size: 24px;
                font-weight: bold;
                margin: 5px 0;
            }}
            .metric-label {{
                color: #6c757d;
                font-size: 14px;
            }}
            .metric-subtext {{
                font-size: 12px;
                color: #6c757d;
                margin-top: 5px;
            }}
            .smaller-text {{
                font-size: 0.9em;
            }}
            .pass-metric {{ border-top: 4px solid #28a745; }}
            .fail-metric {{ border-top: 4px solid #dc3545; }}
            .warn-metric {{ border-top: 4px solid #ffc107; }}
            .neutral-metric {{ border-top: 4px solid #66b2ff; }}
        </style>
        <script>
            function toggleVisibility(id, btn) {{
                var el = document.getElementById(id);
                if (el.style.display === 'none') {{
                    el.style.display = 'block';
                    btn.innerHTML = '-';
                }} else {{
                    el.style.display = 'none';
                    btn.innerHTML = '+';
                }}
            }}
        </script>
    </head>
    <body>
        <header>
        <table style="border: none; width: 100%;">
            <tr>
                <td style="border: none; text-align: left; width: 20%; padding: 5px 0;">
                    <img src="{project_logo}" alt="{project_name}" style="height: 40px; width: auto;">
                </td>
                <td style="border: none; text-align: center; padding: 5px 0;">
                    <h1 style="margin: 0; font-size: 1.5em;">{project_name} - CSV Comparison Report</h1>
                    <p style="margin: 2px 0 0; font-size: 0.9em;"> <strong>Generated:</strong> {report_start_time.strftime('%Y-%m-%d %H:%M:%S')} | <strong>Duration:</strong> {time_taken_str}</p>
                </td>
                <td style="border: none; text-align: right; width: 20%; padding: 5px 0;">
                    <div style="font-size: 1.2em; font-weight: bold; color: { '#28a745' if overall_row_pass >= 99.99 else '#dc3545' }">
                        {overall_row_pass}% Row Pass
                    </div>
                </td>
            </tr>
        </table>
    </header>
    <div class="container">
        <h2>📊 Key Metrics</h2>
        <div class="metrics-container">
            <div class="metric-card pass-metric">
                <div class="metric-value">{overall_row_pass}%</div>
                <div class="metric-label">Overall Row Pass Rate</div>
            </div>
            <div class="metric-card fail-metric">
                <div class="metric-value">{total_row_discrepancies}</div>
                <div class="metric-label">Row Discrepancies</div>
            </div>
            <div class="metric-card neutral-metric">
                <div class="metric-value">{total_engine_rows}</div>
                <div class="metric-label">Total Engine Rows</div>
            </div>
            <div class="metric-card neutral-metric">
                <div class="metric-value">{total_neoprice_rows}</div>
                <div class="metric-label">Total Neoprice Rows</div>
            </div>
            <div class="metric-card warn-metric">
                <div class="metric-value">{total_missing_rows}</div>
                <div class="metric-label">Missing Rows</div>
            </div>
            <div class="metric-card warn-metric">
                <div class="metric-value">{total_extra_rows}</div>
                <div class="metric-label">Extra Rows</div>
            </div>
            <div class="metric-card warn-metric">
                <div class="metric-value">{total_duplicates}</div>
                <div class="metric-label">Duplicate Rows</div>
            </div>
        </div>

       {global_percentage_section}

        <h2>🔍 Comparison Details</h2>
        <ul>
            <li><strong><i class="fas fa-file-alt"></i> Files in Engine:</strong> {source_files_count}</li>
            <li><strong><i class="fas fa-file-alt"></i> Files in Neoprice:</strong> {destination_files_count}</li>
            <li>
                <strong><i class="fas fa-key"></i> Primary Keys:</strong> 
                <button class="toggle-button" onclick="toggleVisibility('primaryKeys', this)">+</button>
                <span id="primaryKeys" style="display:none;">
                    <span class="smaller-text">{', '.join(primary_key_columns)}</span>
                </span>
            </li>
            <li>
                <strong><i class="fas fa-columns"></i> Compared Columns:</strong>
                <button class="toggle-button" onclick="toggleVisibility('comparedColumns', this)">+</button>
                <span id="comparedColumns" style="display:none;">
                    <span class="smaller-text">{"All Columns" if not columns else ', '.join(columns)}</span>
                </span>
            </li>
        </ul>

        <h2>📝 File Comparison Results</h2>
        <table>
            <thead>
                <tr>
                    <th width="20%" nowrap>CSV File</th>
                    <th width="5%">Status</th>
                    <th>Details</th>
                </tr>
            </thead>
            <tbody>
                {comparison_rows}
            </tbody>
        </table>
    </div>
    </body>
    </html>
    """

    with open(output_file, "w", encoding="utf-8") as f:
        f.write(html_template)

'''-----------------------------------
Main Execution Functions
------------------------------------'''
def run_comparison(download_local=True):
    source1_zips = list_zip_files(source_1_prefix, download_local)
    source2_zips = list_zip_files(source_2_prefix, download_local)

    # For testing: limit to first file
    source1_zips = [source1_zips[0]]
    source2_zips = [source2_zips[0]]

    all_csvs_source1 = read_all_csvs_by_source(source1_zips, "source1", download_local, use_multithreading_reading)
    all_csvs_source2 = read_all_csvs_by_source(source2_zips, "source2", download_local, use_multithreading_reading)

    diff_df, summary = compare_all_csvs(all_csvs_source1, all_csvs_source2, use_multithreading_comparision)
    list_files = [len(all_csvs_source1), len(all_csvs_source2)]

    return diff_df, summary, list_files

# Main Execution
if __name__ == "__main__":
    # Setup output file
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    base_name, ext = os.path.splitext(output_file)
    extension = ext if ext else ".html"
    output_file = f"{base_name}_{timestamp}{extension}"
    create_dir(output_dir)
    output_file = os.path.join(output_dir, output_file)

    logging.info('---------------- CSV Comparison Started ----------------')
    start_time = datetime.now()
    diff_df, summary, list_files = run_comparison(download_local=download_local)
    generate_html_report(
        diff_df=diff_df,
        summary=summary,
        report_start_time=start_time,
        output_file=output_file,
        source_files_count=list_files[0],
        destination_files_count=list_files[1],
        primary_key_columns=csv_primary_keys,
        columns=csv_columns,
        project_name=project_name,
        project_logo=project_logo,
        include_passed=include_passed,
        include_missing_files=include_missing_files,
        include_extra_files=include_extra_files,
        global_percentage=global_percentage,
        use_multithreading = True
    )
    logging.info('---------------- CSV Comparison Finished ----------------')
