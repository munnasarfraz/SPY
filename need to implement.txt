import pandas as pd
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
from multiprocessing import Queue, cpu_count
from tqdm import tqdm
import multiprocessing as mp

SENTINEL = None

def reader_thread(csv_name, source1_map, source2_map, queue, read_pbar, overall_pbar):
    zip1, file1 = source1_map[csv_name]
    zip2, file2 = source2_map[csv_name]
    df1 = read_csv_from_local_zip(zip1, file1)
    df2 = read_csv_from_local_zip(zip2, file2)
    queue.put((csv_name, file1, df1, df2))
    read_pbar.update(1)  # Update the reading progress bar
    overall_pbar.update(1)  # Update the overall progress bar

def compare_worker(queue, output_list, compare_pbar, overall_pbar):
    while True:
        item = queue.get()
        if item is SENTINEL:
            break
        csv_name, file1, df1, df2 = item
        diff_df, summary = compare_csvs(df1, df2, file1)
        output_list.append((csv_name, diff_df, summary))
        compare_pbar.update(1)  # Update the comparing progress bar
        overall_pbar.update(1)  # Update the overall progress bar

def run_comparison():
    source1_zips = list_zip_files(source_1_prefix)
    source2_zips = list_zip_files(source_2_prefix)

    source1_csv_map = {}
    source2_csv_map = {}

    build_csv_to_zip_map(source1_zips, 'downloads/source1', source1_csv_map)
    build_csv_to_zip_map(source2_zips, 'downloads/source2', source2_csv_map)

    common_csvs = list(set(source1_csv_map.keys()) & set(source2_csv_map.keys()))
    missing_in_source2 = set(source1_csv_map.keys()) - set(source2_csv_map.keys())
    missing_in_source1 = set(source2_csv_map.keys()) - set(source1_csv_map.keys())

    all_diffs = []
    all_summaries = {}

    print(f"ðŸ”„ Reading and comparing {len(common_csvs)} CSV files in parallel...")

    # Shared queue and manager list
    queue = mp.Queue(maxsize=cpu_count() * 2)
    manager = mp.Manager()
    output_list = manager.list()

    # Progress bars
    with tqdm(total=len(common_csvs), desc="Overall Progress", ncols=100) as overall_pbar, \
         tqdm(total=len(common_csvs), desc="Reading Progress", ncols=100) as read_pbar, \
         tqdm(total=len(common_csvs), desc="Comparing Progress", ncols=100) as compare_pbar:

        # Start workers (processes) for comparison
        num_workers = cpu_count()
        processes = []
        for _ in range(num_workers):
            p = mp.Process(target=compare_worker, args=(queue, output_list, compare_pbar, overall_pbar))
            p.start()
            processes.append(p)

        # Reading threads with tqdm progress for reading
        with ThreadPoolExecutor(max_workers=32) as executor:
            futures = [executor.submit(reader_thread, name, source1_csv_map, source2_csv_map, queue, read_pbar, overall_pbar) for name in common_csvs]
            for future in futures:
                future.result()  # Ensure all reader threads complete

        # Send sentinel to signal end of work
        for _ in range(num_workers):
            queue.put(SENTINEL)

        # Wait for all comparison processes to finish
        for p in processes:
            p.join()

    # Collect results
    for csv_name, diff_df, summary in output_list:
        if not diff_df.empty:
            diff_df['File'] = csv_name
            all_diffs.append(diff_df)
        all_summaries[csv_name] = summary

    if missing_in_source2:
        all_summaries["Missing CSVs in Source2"] = list(missing_in_source2)
    if missing_in_source1:
        all_summaries["Extra CSVs in Source2"] = list(missing_in_source1)

    final_diff_df = pd.concat(all_diffs) if all_diffs else pd.DataFrame()
    list_files = [len(source1_csv_map.keys()), len(source2_csv_map.keys())]
    return final_diff_df, all_summaries, list_files
