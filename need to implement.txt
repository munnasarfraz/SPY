def run_comparison():
#     source1_zips = list_zip_files(source_1_prefix)
#     source2_zips = list_zip_files(source_2_prefix)

#     source1_csv_map = {}
#     source2_csv_map = {}

#     build_csv_to_zip_map(source1_zips, 'downloads/source1', source1_csv_map)
#     build_csv_to_zip_map(source2_zips, 'downloads/source2', source2_csv_map)

#     common_csvs = set(source1_csv_map.keys()) & set(source2_csv_map.keys())
#     missing_in_source2 = set(source1_csv_map.keys()) - set(source2_csv_map.keys())
#     missing_in_source1 = set(source2_csv_map.keys()) - set(source1_csv_map.keys())

#     all_diffs = []
#     all_summaries = {}

#     # Shared lock for tqdm update
#     manager = Manager()
#     counter = manager.Value('i', 0)
#     lock = manager.Lock()

#     # Function for threaded progress
#     def process_pair(csv_name):
#         zip1, file1 = source1_csv_map[csv_name]
#         zip2, file2 = source2_csv_map[csv_name]
#         df1 = read_csv_from_local_zip(zip1, file1)
#         df2 = read_csv_from_local_zip(zip2, file2)
#         result = csv_name, *compare_csvs(df1, df2, file1)

#         # Thread-safe tqdm update
#         with lock:
#             counter.value += 1
#             pbar.update(1)
#         return result

#     print(f"üîç Comparing {len(common_csvs)} common CSV files...")
#     with tqdm(total=len(common_csvs), desc="Overall Comparison Progress") as pbar:
#         if use_multithreading_comparision:
#             with ThreadPoolExecutor(max_workers=64) as executor:
#                 results = list(executor.map(process_pair, common_csvs))
#         else:
#             results = [process_pair(name) for name in common_csvs]

#     for csv_name, diff_df, summary in results:
#         if not diff_df.empty:
#             diff_df['File'] = csv_name
#             all_diffs.append(diff_df)
#         all_summaries[csv_name] = summary

#     if missing_in_source2:
#         all_summaries["Missing CSVs in Source2"] = list(missing_in_source2)
#     if missing_in_source1:
#         all_summaries["Extra CSVs in Source2"] = list(missing_in_source1)

#     final_diff_df = pd.concat(all_diffs) if all_diffs else pd.DataFrame()
#     list_files = [len(source1_csv_map.keys()), len(source2_csv_map.keys())]
#     return final_diff_df, all_summaries, list_files




------------------------------
import pandas as pd
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed
from tqdm import tqdm
import multiprocessing as mp

# Define outside so it's pickleable
def read_pair(csv_name, source1_map, source2_map):
    zip1, file1 = source1_map[csv_name]
    zip2, file2 = source2_map[csv_name]
    df1 = read_csv_from_local_zip(zip1, file1)
    df2 = read_csv_from_local_zip(zip2, file2)
    return (csv_name, file1, df1, df2)

def compare_pair(args):
    csv_name, file1, df1, df2 = args
    return csv_name, *compare_csvs(df1, df2, file1)

def run_comparison():
    source1_zips = list_zip_files(source_1_prefix)
    source2_zips = list_zip_files(source_2_prefix)

    source1_csv_map = {}
    source2_csv_map = {}

    build_csv_to_zip_map(source1_zips, 'downloads/source1', source1_csv_map)
    build_csv_to_zip_map(source2_zips, 'downloads/source2', source2_csv_map)

    common_csvs = set(source1_csv_map.keys()) & set(source2_csv_map.keys())
    missing_in_source2 = set(source1_csv_map.keys()) - set(source2_csv_map.keys())
    missing_in_source1 = set(source2_csv_map.keys()) - set(source1_csv_map.keys())

    all_diffs = []
    all_summaries = {}

    print(f"üîç Reading and comparing {len(common_csvs)} CSV files...")

    # Phase 1: Read using threads
    read_results = []
    with ThreadPoolExecutor(max_workers=32) as reader_pool:
        futures = [reader_pool.submit(read_pair, name, source1_csv_map, source2_csv_map) for name in common_csvs]
        for f in tqdm(as_completed(futures), total=len(futures), desc="Reading CSVs"):
            read_results.append(f.result())

    # Phase 2: Compare using processes
    with ProcessPoolExecutor(max_workers=mp.cpu_count()) as compare_pool:
        compare_futures = [compare_pool.submit(compare_pair, args) for args in read_results]
        for f in tqdm(as_completed(compare_futures), total=len(compare_futures), desc="Comparing CSVs"):
            csv_name, diff_df, summary = f.result()
            if not diff_df.empty:
                diff_df['File'] = csv_name
                all_diffs.append(diff_df)
            all_summaries[csv_name] = summary

    # Add missing file info
    if missing_in_source2:
        all_summaries["Missing CSVs in Source2"] = list(missing_in_source2)
    if missing_in_source1:
        all_summaries["Extra CSVs in Source2"] = list(missing_in_source1)

    final_diff_df = pd.concat(all_diffs) if all_diffs else pd.DataFrame()
    list_files = [len(source1_csv_map.keys()), len(source2_csv_map.keys())]
    return final_diff_df, all_summaries, list_files

------------------------------------------------------------------
import pandas as pd
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
from multiprocessing import Queue, cpu_count
from tqdm import tqdm
import multiprocessing as mp

SENTINEL = None

def reader_thread(csv_name, source1_map, source2_map, queue):
    zip1, file1 = source1_map[csv_name]
    zip2, file2 = source2_map[csv_name]
    df1 = read_csv_from_local_zip(zip1, file1)
    df2 = read_csv_from_local_zip(zip2, file2)
    queue.put((csv_name, file1, df1, df2))

def compare_worker(queue, output_list):
    while True:
        item = queue.get()
        if item is SENTINEL:
            break
        csv_name, file1, df1, df2 = item
        diff_df, summary = compare_csvs(df1, df2, file1)
        output_list.append((csv_name, diff_df, summary))

def run_comparison():
    source1_zips = list_zip_files(source_1_prefix)
    source2_zips = list_zip_files(source_2_prefix)

    source1_csv_map = {}
    source2_csv_map = {}

    build_csv_to_zip_map(source1_zips, 'downloads/source1', source1_csv_map)
    build_csv_to_zip_map(source2_zips, 'downloads/source2', source2_csv_map)

    common_csvs = list(set(source1_csv_map.keys()) & set(source2_csv_map.keys()))
    missing_in_source2 = set(source1_csv_map.keys()) - set(source2_csv_map.keys())
    missing_in_source1 = set(source2_csv_map.keys()) - set(source1_csv_map.keys())

    all_diffs = []
    all_summaries = {}

    print(f"üîÑ Reading and comparing {len(common_csvs)} CSV files in parallel...")

    # Shared queue and manager list
    queue = mp.Queue(maxsize=cpu_count() * 2)
    manager = mp.Manager()
    output_list = manager.list()

    # Start workers (processes)
    num_workers = cpu_count()
    processes = []
    for _ in range(num_workers):
        p = mp.Process(target=compare_worker, args=(queue, output_list))
        p.start()
        processes.append(p)

    # Reading threads
    with ThreadPoolExecutor(max_workers=32) as executor:
        list(tqdm(executor.map(lambda name: reader_thread(name, source1_csv_map, source2_csv_map, queue), common_csvs), total=len(common_csvs), desc="Reading Files"))

    # Send sentinel to signal end of work
    for _ in range(num_workers):
        queue.put(SENTINEL)

    # Wait for all workers to finish
    for p in processes:
        p.join()

    # Collect results
    for csv_name, diff_df, summary in output_list:
        if not diff_df.empty:
            diff_df['File'] = csv_name
            all_diffs.append(diff_df)
        all_summaries[csv_name] = summary

    if missing_in_source2:
        all_summaries["Missing CSVs in Source2"] = list(missing_in_source2)
    if missing_in_source1:
        all_summaries["Extra CSVs in Source2"] = list(missing_in_source1)

    final_diff_df = pd.concat(all_diffs) if all_diffs else pd.DataFrame()
    list_files = [len(source1_csv_map.keys()), len(source2_csv_map.keys())]
    return final_diff_df, all_summaries, list_files


----------------------------------------------------------------------
import pandas as pd
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
from multiprocessing import Queue, cpu_count
from tqdm import tqdm
import multiprocessing as mp

SENTINEL = None

def reader_thread(csv_name, source1_map, source2_map, queue, progress_bar):
    zip1, file1 = source1_map[csv_name]
    zip2, file2 = source2_map[csv_name]
    df1 = read_csv_from_local_zip(zip1, file1)
    df2 = read_csv_from_local_zip(zip2, file2)
    queue.put((csv_name, file1, df1, df2))
    
    # Increment the progress bar after reading the file
    progress_bar.update(1)

def compare_worker(queue, output_list, progress_bar):
    while True:
        item = queue.get()
        if item is SENTINEL:
            break
        csv_name, file1, df1, df2 = item
        diff_df, summary = compare_csvs(df1, df2, file1)
        output_list.append((csv_name, diff_df, summary))
        
        # Increment the progress bar after comparing the file
        progress_bar.update(1)

def run_comparison():
    source1_zips = list_zip_files(source_1_prefix)
    source2_zips = list_zip_files(source_2_prefix)

    source1_csv_map = {}
    source2_csv_map = {}

    build_csv_to_zip_map(source1_zips, 'downloads/source1', source1_csv_map)
    build_csv_to_zip_map(source2_zips, 'downloads/source2', source2_csv_map)

    common_csvs = list(set(source1_csv_map.keys()) & set(source2_csv_map.keys()))
    missing_in_source2 = set(source1_csv_map.keys()) - set(source2_csv_map.keys())
    missing_in_source1 = set(source2_csv_map.keys()) - set(source1_csv_map.keys())

    all_diffs = []
    all_summaries = {}

    print(f"üîÑ Reading and comparing {len(common_csvs)} CSV files in parallel...")

    # Shared queue and manager list
    queue = mp.Queue(maxsize=cpu_count() * 4)  # Larger queue size
    manager = mp.Manager()
    output_list = manager.list()

    # Progress bar for the overall process (reading + comparing)
    total_files = len(common_csvs)
    progress_bar = tqdm(total=total_files * 2, desc="Overall Progress", ncols=100)

    # Start workers (processes)
    num_workers = 8  # Increase number of workers for CPU-bound tasks (try 8 or 16)
    processes = []
    for _ in range(num_workers):
        p = mp.Process(target=compare_worker, args=(queue, output_list, progress_bar))
        p.start()
        processes.append(p)

    # Reading threads (increase max_workers to 64 or higher)
    with ThreadPoolExecutor(max_workers=64) as executor:  # 64 workers for I/O-bound
        list(tqdm(executor.map(lambda name: reader_thread(name, source1_csv_map, source2_csv_map, queue, progress_bar), common_csvs), total=len(common_csvs), desc="Reading Files"))

    # Send sentinel to signal end of work
    for _ in range(num_workers):
        queue.put(SENTINEL)

    # Wait for all workers to finish
    for p in processes:
        p.join()

    # Collect results
    for csv_name, diff_df, summary in output_list:
        if not diff_df.empty:
            diff_df['File'] = csv_name
            all_diffs.append(diff_df)
        all_summaries[csv_name] = summary

    if missing_in_source2:
        all_summaries["Missing CSVs in Source2"] = list(missing_in_source2)
    if missing_in_source1:
        all_summaries["Extra CSVs in Source2"] = list(missing_in_source1)

    final_diff_df = pd.concat(all_diffs) if all_diffs else pd.DataFrame()
    list_files = [len(source1_csv_map.keys()), len(source2_csv_map.keys())]
    
    # Close the progress bar after all work is done
    progress_bar.close()

    return final_diff_df, all_summaries, list_files


