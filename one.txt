import pandas as pd
import zipfile
import os
from concurrent.futures import ThreadPoolExecutor
from multiprocessing import Queue, cpu_count, Process, Manager
from tqdm import tqdm
import multiprocessing as mp
import time

SENTINEL = None
BATCH_SIZE = 4

# Utility: Chunk list into batches
def chunk_list(lst, chunk_size):
    for i in range(0, len(lst), chunk_size):
        yield lst[i:i + chunk_size]

# Extract all ZIPs to disk once
def extract_all_zips(zip_map, extract_root_dir):
    extracted_files = {}
    os.makedirs(extract_root_dir, exist_ok=True)

    extracted_zips = set()
    for csv_name, (zip_path, file_name) in zip_map.items():
        zip_name = os.path.splitext(os.path.basename(zip_path))[0]
        extract_dir = os.path.join(extract_root_dir, zip_name)
        extracted_file_path = os.path.join(extract_dir, file_name)

        if not os.path.exists(extracted_file_path):
            if zip_path not in extracted_zips:
                os.makedirs(extract_dir, exist_ok=True)
                with zipfile.ZipFile(zip_path, 'r') as zip_ref:
                    zip_ref.extractall(extract_dir)
                extracted_zips.add(zip_path)

        extracted_files[csv_name] = extracted_file_path

    return extracted_files

# Thread that reads batches of CSVs and pushes to the queue
def reader_batch_thread(csv_names, source1_map, source2_map, queue, read_pbar, overall_pbar, extract=False):
    batch = []
    for csv_name in csv_names:
        if extract:
            file1_path = source1_map[csv_name]
            file2_path = source2_map[csv_name]
            df1 = pd.read_csv(file1_path)
            df2 = pd.read_csv(file2_path)
        else:
            zip1, file1 = source1_map[csv_name]
            zip2, file2 = source2_map[csv_name]
            df1 = read_csv_from_local_zip(zip1, file1)
            df2 = read_csv_from_local_zip(zip2, file2)

        batch.append((csv_name, file1, df1, df2))
        read_pbar.update(1)
        overall_pbar.update(1)
    queue.put(batch)

# Compare worker: processes batches
def compare_worker(queue, output_list, compare_pbar, overall_pbar):
    while True:
        batch = queue.get()
        if batch is SENTINEL:
            break
        for csv_name, file1, df1, df2 in batch:
            start = time.time()
            print(f"[PID {os.getpid()}] Comparing: {csv_name}")
            diff_df, summary = compare_csvs(df1, df2, file1)
            print(f"[PID {os.getpid()}] Done {csv_name} in {time.time() - start:.2f}s")
            output_list.append((csv_name, diff_df, summary))
            compare_pbar.update(1)
            overall_pbar.update(1)

# Placeholder: Define your CSV reading and comparison logic
def read_csv_from_local_zip(zip_path, file_name):
    with zipfile.ZipFile(zip_path, 'r') as z:
        with z.open(file_name) as f:
            return pd.read_csv(f)

def compare_csvs(df1, df2, file_name):
    # Dummy implementation for structure
    diff_df = pd.concat([df1, df2]).drop_duplicates(keep=False)
    summary = {'rows_in_df1': len(df1), 'rows_in_df2': len(df2), 'diff_rows': len(diff_df)}
    return diff_df, summary

# Example file listing and map builder
def list_zip_files(folder):
    return [os.path.join(folder, f) for f in os.listdir(folder) if f.endswith('.zip')]

def build_csv_to_zip_map(zip_paths, base_folder, csv_map):
    for zip_path in zip_paths:
        with zipfile.ZipFile(zip_path, 'r') as z:
            for name in z.namelist():
                if name.endswith('.csv'):
                    csv_map[name] = (zip_path, name)
